%Auteurs : Nicolas Englebert
\documentclass[british,french,11pt, a4paper, openany]{book}

% Règles de bonne pratiques :
% https://fr.wikibooks.org/wiki/LaTeX/Gestion_des_gros_documents

\input{../../Builder/preambule.tex}

% Attributs
\NomduCours{Algèbre linéaire}{MATH-H-101}
\addauteur{Nicolas}{Englebert}
\addprofesseur{Dominique}{Buset}
\annee{2013}{2014}

% Document
\begin{document}
\def\equationautorefname~#1\null{%
	(#1)\null
}
\newcommand{\pscal}[2]{\left\langle {#1} , {#2} \right\rangle}

%%%%%%%%%%%%%%%%%
% Préliminaires %
%%%%%%%%%%%%%%%%%
\frontmatter
\input{../../Builder/titlepage/titlepage.tex}
\input{../../Builder/APropos.tex}
\tableofcontents
%Si abstract, \input ici

%%%%%%%%%%%%%%%%%%%%%
% Contenu principal %
%%%%%%%%%%%%%%%%%%%%%
\mainmatter
\chapter{Structure}
\subsection{Relation}
Étant donné deux ensemble A et B, on appelle \textit{relation} de A vers B tout ensemble de couple dont l'origine appartient à A, et l'extrêmité à B :
$$ \forall a, b \in f \subseteq A \times B : b =f(a) = l'image\ de\ A\ par\ f$$
\textbf{Vocabulaire}\\
\begin{description}
	\item[Application] Relation $A \longrightarrow B$ tel que A a pour image un seul élément de B par cette application
	\item[Injection] $\forall x,y \in A : [x \neq y \Rightarrow f(x) \neq f(y)]$
	\item[Surjection] $\forall b \in B, \exists a \in A : b = f(a)$
	\item[Bijection] $\forall b \in B, \exists a! \in A : b = f(a)$
\end{description}
On appelle relation d'équivalence, toute relation :
\begin{itemize}
	\item[Réflexive] $\forall a \in E : a \Re a$
	\item[Symétrique] $\forall a, b \in E, a \Re b = b \Re a$
	\item[Transitive] $\forall a, b, c \in E, a \Re b\ et\ b \Re c \Rightarrow a \Re c$
\end{itemize}
\ \ \\
On appelle relation d'ordre, toute relation :
\begin{itemize}
	\item[Réflexive] $\forall a \in E : a \Re a$
	\item[antisymétrique] $\forall a, b \in E, a \Re b\ et\ b \Re a \Rightarrow a = b$
	\item[Transitive] $\forall a, b, c \in E, a \Re b\ et\ b \Re c \Rightarrow a \Re c$
\end{itemize}\ \\
On dira qu'une relation d'ordre $\wp$ dans un ensemble E forme un ordre total ssi $\forall a, b \in E, a \wp b\ ou\ b \wp a$\\
\textbf{Attention :} Il n'y a pas d'ordre total dans $\mathbb{C}$.


\section{Groupe}
\subsection{Définition générale}
Etant donné un ensemble E, une loi $\clubsuit$ sur E est une application :
$$\clubsuit: E \times E \rightarrow E : (x,y) \rightarrow x\ \clubsuit\ y$$
Pour être un groupe, il faut vérifier les 4 propriétés suivantes : 
\begin{itemize}
	\item[$\clubsuit$ est \textit{interne} dans E] \ \ \ \ \ \ \ \ \ \  \ $\forall x, y \in E : x\ \clubsuit\ y \in E$
	\item[$\clubsuit$ est \textit{associative} dans E]\ \ \ \ \ \ $\forall x, y, z \in E : (x\ \clubsuit\ y)\clubsuit\ z =  x\ \clubsuit(y\ \clubsuit\ z)$
	\item[$\exists$ un \textit{neutre} pour $\clubsuit$ dans E] \ \ \  $\exists n \in E\ |\ \forall x \in E : x\ \clubsuit\ n = x = n\ \clubsuit\ x$
	\item[$\clubsuit$ est \textit{symétrisable} dans E]\ \ \ \ \ $\forall x \in E\ |\ \exists x' \in E : x\ \clubsuit\ x' = n = x'\ \clubsuit\ x$
\end{itemize}
\ \\
On dira qu'un ensemble E muni d'une loi $\clubsuit$ est un groupe \textit{commutatif} (ou \textit{abélien}) ssi :
$$E, \clubsuit\ est\ un\ groupe\ et\ [\forall x, y \in E, x\ \clubsuit\ y = y\ \clubsuit\ x$$
	\ \\
	Un groupe est \textit{d'ordre $n$} ssi $|E| = \#E = n$.\\
	Un élément $a$ d'un ensemble E muni d'une loi $\clubsuit$ est un \textit{absorbant pour $\clubsuit$} dans E ssi :
	$$a \in E\ et\ [\forall x, y \in E : x\ \clubsuit\ a = a = a\ \clubsuit\ x]$$
	\textbf{Attention :} Il y a uniticité du neutre et du symétrique (de chaque élément) pour une loi donnée dans un groupe.\\ \\
	Une autre propriété importante est la \textit{simplifiabilité} (préciser le côté !) :
	$$\forall a, b, c \in E : (a\ \clubsuit\ c) = (b\ \clubsuit\ c) \Leftrightarrow a = b\ (Simplifiabilité\ à\ droite)$$
	$$\forall a, b, c \in E : (c\ \clubsuit\ a) = (c\ \clubsuit\ b) \Leftrightarrow a = b\ (Simplifiabilité\ à\ gauche)$$
	
	\subsection{Isomorphisme de groupe}
	Deux groupes E, $\clubsuit$ et G, $\bigstar$ sont \textit{isomorphes} $\Leftrightarrow$ il existe une bijection $\delta : E \rightarrow G$ telle que : 
	$$\forall a, b \in E : \delta (a\ \clubsuit\ b) = \delta(a)\ \bigstar\ \delta(b)$$
	On dira alors que la bijection $\delta$ est un isomorphisme entre les groupes E et G.\\
	\textit{Un isomorphisme de groupe est une bijection conservant la structure du groupe}
	
	\subsection{Sous-groupes d'un groupe}
	Soit E, $\clubsuit$ un groupe. H, $\clubsuit$ est un \textit{sous-groupe} de  E, $\clubsuit$ ssi :
	\begin{itemize}
		\item[1] H est un \textit{sous-ensemble} de E
		\item[2] $\forall x, y \in H : x\ \clubsuit\ y\ \in H$
		\item[3] Le \textit{neutre} de E pour $\clubsuit \in E$
		\item[4] $\forall x \in H$ : le symétrique de $x$ pour $\clubsuit$ dans E est un élément de H
	\end{itemize}
	\ \\
	Un petit théorème en passant : \textit{Théorème de Lagrange} : Si H, $\clubsuit$ est un sous-groupe \textbf{fini} de E, $\clubsuit$ alors l'ordre de H divise l'ordre de E.
	
	\section{Anneau}
	Soit A, un ensemble munis de deux lois $\clubsuit$ et $\bigstar$.\\
	A, $\clubsuit$, $\bigstar$ est un anneau ssi :
	\begin{itemize}
		\item[1] A, $\clubsuit$ est un groupe \textit{commutatif}
		\item[2] $\bigstar$ est \textit{interne} et \textit{associatif} dans A
		\item[3] $\bigstar$ distribue $\clubsuit$ dans A
	\end{itemize}
	\ \\
	On dira que A, $\clubsuit$, $\bigstar$ est un anneau \textit{unital} ssi A, $\clubsuit$, $\bigstar$ est un anneau et s'il existe un \textit{neutre pour $\bigstar$} différent du neutre pour $\clubsuit$ dans A.
	
	\section{Corps - champ}
	Soit K, $\clubsuit$, $\bigstar$ un corps ssi : 
	\begin{itemize}
		\item[1] K, $\clubsuit$ est \textit{interne} et \textit{associatif} dans A
		\item[2] $K_{n}$, $bigstar$ est un groupe \textit{commutatif} (ou $n$ est neutre pour $\clubsuit$ dans K)
		      
		\item[3] $\bigstar$ distribue $\clubsuit$ dans A
	\end{itemize}
	\ \\
	
	Soit K, un ensemble muni de deux lois $\clubsuit$ et  $\bigstar$.\\
	K, $\clubsuit$, $\bigstar$ un corps commutatif ou \textit{champ} ssi :
	\begin{itemize}
		\item[1] K, $\clubsuit$, $\bigstar$ est un corps
		\item[2] $\bigstar$ est \textit{commutative} dans K
	\end{itemize}
	
	\chapter{Espaces Vectoriels}
	\section{Définition}
	A INCLURE.
	
	\section{Exemples d'EV}
	\textit{Cf. cours}
	
	\section{Prorpiétés des espaces vectoriels}
	Si K, V, + est un espace vectoriel, alors :
	\begin{enumerate}
		\item $\forall x \in V : 0.\vec{x} = \vec{0}$
		\item $\forall \lambda \in K : \lambda.\vec{0} = \vec{0}$
		\item $\forall \lambda \in K, \forall \vec{x} \in V : \lambda \vec{0} \Rightarrow \lambda = 0\ ou\ \vec{x} = \vec{0}$
		\item $\forall \lambda \in K, \forall \vec{x} \in V : \lambda(- \vec{x}) = -\lambda\vec{x}$
		\item $\forall \lambda_{i} \in K, \forall \vec{x} \in V : (\sum_{1}^{i} \lambda_{i})\vec{x} = \sum_{1}^{i}(\lambda_{i}\vec{x})$
		\item $\forall \lambda_{i} \in K, \forall \vec{x} \in V : (\sum_{1}^{i} \vec{x}_{i})\lambda = \sum_{1}^{i}(\lambda\vec{x_{i}})$
	\end{enumerate}
	
	\section{Sous-espaces vectoriels}
	Soit W, un sous-ensemble d'un espace vectoriel K, V, + défini sur un corps K\\
	K, W, + est un \textit{sous-espace vectoriel} ssi :
	\begin{enumerate}
		\item W est non vide (signifie $\vec{0} \in W$)
		\item $\forall \vec{x}, \vec{y} \in W = \vec{x} + \vec{y} \in W$
		\item $\forall \vec{x} \in W, \forall \lambda \in K : \lambda\vec{x} \in W$
	\end{enumerate}
	\textbf{Attention :} Les lois et corps doivent être identiques !\\
	\textit{NB :} Un espace vectoriel est le plus grand sous-vectoriel de lui-même. De même K, $\{\vec{0}\}$, + est le plus petit des EV. Ces deux SV sont dit \textit{triviaux}.\\
	\textit{NB.2 } L'intersection de deux sous-espaces vectoriel et un SEV.
	
	\subsection{Lien avec les équations linéaire homogènes}
	L'ensemble des solution d'une équation linéaire homogène à $n$ inconnues et à coefficient dans un corps K est un sous-espace vectoriel de $K, K^{n}, +$.
	
	\subsection{Lien avec les systèmes d'équations linéaires homogènes}
	L'ensemble des solution d'un système d'équations linéaires homogènes est un sous-espace vectoriel de $K^{n}$ défini sur un corps K (commutatif).
	
	\section{Somme de SEV}
	Soit K, V, + un espace vectoriel et $W_{1}, W_{2}$ deux SEV de V. On appelle \textit{somme de $W_{1}, W_{2}$} l'ensemble défini par :
	$$W_{1}, W_{2} = \{\vec{w_{1}} + \vec{w_{2}}\ |\ \vec{w_{1}} \in W_{1}, \vec{w_{2}} \in W_{2}\}$$
	
	Soit V, + un  EV sur un corps K et $W_{1}, W_{2}$ deux SEV de V. On dira que V est la somme directe (notée $\bigoplus$) de $W_{1}, W_{2}$ ssi
	$$[V = W_{1} + W_{2}\ et\ W_{1} \cap W_{2} = \{\vec{0}\}]$$
	
	\section{Isomorphisme d'espaces vectoriels}
	Soient K, V, + et K, W, + deux EV défini sur le \textit{même corps}.\\
	V est \textit{isomorphe} ) W (V $\cong $ W) ssi il existe une bijection $\sigma V \rightarrow W$ telle que : 
	\begin{enumerate}
		\item $\forall \vec{x}, \vec{y} \in V : \sigma(\vec{x} + \vec{y}) = \sigma(\vec{x}) + \sigma(\vec{y})$
		\item $\forall \vec{x} \in V, \forall \lambda \in K : \sigma(\lambda\vec{x}) = \lambda\sigma(\vec{x})$
	\end{enumerate}
	\textbf{Attention :} Il s'agit d'une question typique de la partie théorique de l'examen de janvier.
	
	\section{Parties génératrices}
	Si K, V, + est un espace vectoriel et si P est un sous-ensemble de V, alors :\\
	\textit{L(P)} est le sous-espace de V, engendré par P.\\\\
	\textit{NB :} Toute partie contenant une partie génératrice est une partie génératrice. Une partie génératrice est dite minimale, s'il n'existe pas de sous ensemble inclus dans P qui soit également génératrice.
	
	\section{Combinaisons linéaires de vecteurs}
	Si X est une partie d'un espace vectoriel K, V, +, on appelle \textit{combinaison linéaire des vecteurs de X} ou \textit{combili des vecteurs de X} tout vecteur V de la forme :
	$$\lambda_{1}\vec{x_{1}} + \lambda_{2}\vec{x_{2}} + ... + \lambda_{n}\vec{x_{n}}$$
	où les $\vec{x_{i}}$ sont les éléments de X \textbf{en nombre fini} et $\lambda_{i}$ sont des élément du corps K.\\\\
	\textit{Théorème :} Pour toute partie non vide X d'un espace vectoriel K, V, + le sous-espace L(X) engendré par X est l'ensemble des combili des vecteurs de X.
	
	\section{Parties libres de vecteurs}
	\textit{Voir horrible définition dans le cours (\textbf{Attention :} AVC possible)}.\\
	Notons tout de même que si X est un sous-ensemble d'un EV K, V, + contenant $\vec{0}$ alors X \textbf{n'est pas} une partie libre de V.\\
	\textit{NB :} Si X est une partie libre d'un EV, alors tout sous-ensemble de X est une partie libre de V (L'ensemble vide également)\\\\
	\textbf{Théorème } : Soit X, une partie libre d'un EV K, V, +.\\
	X est une PL ssi :
	$$\forall \vec{x_{i}} \in X, \forall \lambda_{i} \in K$$
	$$[\sum_{i=1}^{n} \lambda_{i}\vec{x_{i}} = \vec{0} \Rightarrow \lambda_{1}, \lambda_{2}, ... \lambda_{n} = 0]$$
	Deux vecteurs sont LI s'ils forment une PL de V.
	
	\section{Bases et dimension}
	Une partie sera une base $\Leftrightarrow$ celle-ci est à la fois libre et génératrice. On dira qu'une base est une \textit{partie libre minimale} et une \textit{partie génératrice maximale}.\\\\
	\textbf{Théorème :} Si K, V, + est un espace vectoriel, si L est une PL de V et si G est une PG de V contenant L, alors il existe une base B de V telle que $L \subseteq B \subseteq G$.\\
	\textit{Ce théorème permet d'étendre une partie libre pour former une base}.\\\\
	Notons également :
	\begin{itemize}
		\item Tout EV possède une base.
		\item S'il existe une base finie de n élément, toute base comporte n élément.
		\item Les bases d'un même EV ont le même cardinal.
		\item Toute PL a au plus n éléments.
		\item Toute PG a au max n éléments.
	\end{itemize}
	\ \\
	Si la dimension d'un EV est finie et vaut n, alors :
	\begin{itemize}
		\item Toute partie libre de n éléments est une base de cet EV.
		\item Toute partie génératrice de n éléments est une base de cet EV.
	\end{itemize}
	\ \\
	\textit{NB :} $dim(W_{1} + W_{2}) = dim(W_{1}) + dim(W_{2}) - dim(W_{1} \cap W_{2})$\\\\
	Si B est une base d'un espace vectoriel, alors tout vecteur de V s'exprime \textbf{d'une et une seule manière} comme combili d'un nombre \textbf{fini} des vecteurs de B.
	
	\section{Base canonique}
	Liste de bases à connaître par coeur ! \textit{Cf. cours}
	
	\section{Écriture matricielle d'une vecteur dans une base donnée}
	$\forall x \in V : \exists 1!(\lambda_{1}, \lambda_{2}, ..., \lambda_{n}) \in K^{n}\ |\ \vec{x} = \sum_{i=1}^{n} \lambda_{i}\vec{x_{i}}$, les vecteurs $\vec{e_{i}}$ étant vecteurs de B.\\
	\textbf{Convention de notation :}\\
	$$\vec{x} = \sum_{i=1}^{n}  \lambda_{i}\vec{e_{i}} = \lambda_{1}\vec{e_{1}}, \lambda_{2}\vec{e_{2}}, ..., \lambda_{n}\vec{e_{n}} = (\vec{e_{1}}\ \vec{e_{2}}\ \ \ \ \vec{e_{n}})\begin{pmatrix} 
	\lambda_1 \\ 
	\lambda_2 \\ 
	\vdots\\ 
	\lambda_2 \end{pmatrix}$$
	On nommera $\begin{pmatrix} 
	\lambda_1 \\ 
	\lambda_2 \\ 
	\vdots\\ 
	\lambda_2 \end{pmatrix}$ \textit{matrice des composantes de $\vec{x}$ dans la base B. Notation : $X_B$}.
	\\
	\textbf{Attention :} Ne pas convondre un vecteur et sa matrice de 
	composantes.
	
	
	\section{Changement de bases et de composantes}
	(\textit{On suppose K, V, + est un EV de dimension finie sur un corps K commutatif})\\
	\subsection{Matrice de changement de base de $e$ vers $\epsilon$}
	Cette section (et les deux suivantes) étant principalement pratique, je ne me contenterai ici que de reprendre les notations (\textit{cf. TP5/6} :) )\\
	$P_e^a$ se lit :
	\begin{itemize}
		\item Matrice de changement de base de $e$ vers $a$ (On "monte" dans les \textbf{B}ase comme \textbf{B}uzz l'éclair).
		\item Matrice de changement de composante de $a$ vers $e$ (On "descend", on \textbf{C}reuse les \textbf{C}omposantes).
	\end{itemize}
	\subsection{Détermination de la patrice $P_\epsilon^e$}
	$$P_\epsilon^e = (P_e^\epsilon)^{-1}$$
	
	\subsection{Détermination de $X_\epsilon$ des vecteurs de $\vec{x}$ dans la base $\epsilon$ à partir de la matrice $X_e$ des composantes de $\vec{x}$ dans la base $e$}
	$$X_\epsilon = P_\epsilon^e X_e$$
	où $P_\epsilon^e$ est la matrice de changement de base de $\epsilon$ vers $e$.
	
	\section{Droites, plans et hyperplans vectoriels}
	\subsection{Droite vectorielle}
	Si K, V, + est un espace vectoriel de dimension au moins 1, alors on appelle une \textit{droite (vectorielle)} de V tout SEV de V de dimension 1.
	
	\subsection{Plan vectoriel}
	Si K, V, + est un espace vectoriel de dimension au moins 2, alors on appelle une \textit{plan (vectoriel)} de V tout SEV de V de dimension 2.
	
	\subsection{Hyperplan vectoriel}
	Soit K, V, + un espace vectoriel. Un sous-espace vectoriel est un \textit{hyperplan (vectoriel)} de V $\Leftrightarrow$ H est un sous-espace vectoriel propre et maximal de V.\\
	\begin{itemize}
		\item[Maximal] : $\Leftrightarrow \nexists$ de SEV S de V tel que $H\subset S \subset V$
		\item[Propre] : $H \neq V$
	\end{itemize}
	\ \\
	\textit{La fin du chapitre est à lire à titre informatif}.
	
	
	\chapter{Applications linéaires}
	\section{Définition}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un même corps K, on appelle \textit{application linéaire} de V dans W tout application $\sigma : V \rightarrow W$ telle que :
	$$\forall \vec{x}, \vec{y} \in V : \sigma(\vec{x} + \vec{y}) = \sigma(\vec{x}) + \sigma(\vec{y})$$
	$$\forall \lambda \in K, \forall \vec{x} \in V : \sigma(\lambda\vec{x}) = \lambda \sigma(\vec{x})$$
	\textbf{Cas particuliers importants :} 
	\begin{enumerate}
		\item Si $\sigma$ est une application linéaires de V dans W, avec V=W, alors $\sigma$ es appelé \textit{opérateur linéaire} ou \textit{endomorphisme} de V.
		\item Si $\sigma$ est une application de V dans W, avec W=K, alors $\sigma$ est une \textit{forme linéaire} ou \textit{covecteur} définie sur V.
		\item Si $\sigma$ est une bijection linéaire de V dans W, alors $\sigma$ est un \textit{isomorphisme} de V sur W.
		\item Si $\sigma$ est un opérateur linéaire de V \textit{et} une bijection, alors $\sigma$ est un \textit{automorphisme} de V.
	\end{enumerate}
	
	\section{Matrices d'une AL dans les bases données}
	
	Encore une fois, c'est principalement pratique (\textit{cf TP6}). Néanmoins : \\
	$A_\epsilon^e$, matrice de $\alpha$ dans les bases $e$ et $\epsilon$, est une matrice de p ligne(cardinal de la base $\epsilon$ et de n colonnes (cardinal de la base $e$).\\\\
	\textbf{Attention :} Les matrices de rotations sont à étudier par coeur! Ne pas oublier de donner les bases, sinon cela n'a pas de sens ! 
	
	\section{Composante de l'image d'un vecteur par une AL}
	Même remarque qu'au point précédent, partie essentiellement pratique.
	
	\section{Composée de deux AL}
	Si K, V, +, K, W, + et K, S, + sont trois espaces vectoriels définis sur un même corps K et si $\alpha : V \rightarrow W$ et $\beta : W \rightarrow S$ sont deux applications linéaires, alors $\beta  \circ  \alpha$ est une application linéaire.
	
	\section{Matrices d'AL et changement de base}
	Partie pratique. (\textit{Cf. TP6})
	
	\section{Espace vectoriel des AL}
	\subsection{Théorème 1}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un coprs K \textit{commutatif} et si $L(V, W)$ est l'ensemble de toutes les applications linéaire $V \rightarrow W$, alors $K, L(V,W), +$ est un espace vectoriel sur K.\\\\
	\textbf{Attention :} Regarder attentivement la définition de l'addition vectorielle et de la multiplication par un scalaire.
	
	\subsection{Théorème 4}
	Si K, V, + est un espace vectoriel de dimension \textit{finie} $n$, défini sur un corps K \textit{commutatif}, et si K, W, + est un espace vectoriel de dimension \textit{finie} p défini sur le même corps K, alors :
	$$K, L(V, W), + \cong K, K^{p \times n}, +$$
	
	\textbf{Corolaire}
	$$dim(L(V,W)) = pn$$
	
	\section{Noyeau d'une AL}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K \textit{commutatif}, et si $\alpha : V \rightarrow W$ est une application linéaire, alors le \textbf{noyau de $\alpha$}, noté \textit{ker $\alpha$}, est l'ensemble :
	$$ker\ \alpha = \{\vec{x} \in V\ |\ \alpha(\vec{x}) = \vec{0}\ (de\ W)\}$$
	
	\subsection{Théorème 1}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K commutatif et si $\alpha V \rightarrow W$ est une application linéaire, alors le vecteur nul de V appartient toujours au noyau de $\alpha$.
	
	\subsection{Théorème 2}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K commutatif, et si $\alpha : V \rightarrow W$ est une AL, alors :
	$$\alpha\ est\ injective\ \Leftrightarrow ker\ \alpha = \{\vec{0}\}$$
	
	\subsection{Théorème 3}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors K, $ker\ \alpha$, + est un sous-espace vectoriel de K, V, +
	
	\subsection{Autre définition}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors on appelle \textit{nullité de $\alpha$} la dimension de $\ker\ \alpha$.
	
	\section{Image d'une AL}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K \textit{commutatif},  et si $\alpha : V \rightarrow W$ est une AL, alors on appelle \textit{image de $\alpha$}, noté $Im\ \alpha$ l'ensemble :
	$$Im\ \alpha = \{\vec{y} \in W\ |\ \exists \vec{x} \in V\ avec\ \alpha(\vec{x}) = \vec{y}\}$$
	
	\subsection{Théorème 4}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL alors :
	$$\alpha\ est\ surjective\ \Leftrightarrow\ Im\ \alpha = W$$
	
	\textbf{Corolaire :}\\
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL alors $\alpha$ est \textit{bijective} $\Leftrightarrow$
	$$ker\ \alpha = \{\vec{0}\}\ et\ Im\ \alpha = W$$
	
	\subsection{Théorème 5}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K, alors l'image d'un sous-espace vectoriel $S$ de $V$ par une AL$\alpha : V \rightarrow W$ est un sous-espace vectoriel de $W$.
	\\\\
	\textbf{Corolaire :}\\
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors K, Im $\alpha$, + est un SEV de K, W, +.
	
	\subsection{Autre définition}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors on appelle \textit{rang de $\alpha$}, la dimension de Im $\alpha$.
	
	\subsection{Théorème 6 (Très important)}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, et si K, V, + est de dimension \textit{finie} $n$ alors :
	\begin{center}
		L'image d'une base par $\alpha$ est une partie génératrice de Im $\alpha$.
	\end{center}
	
	\section{Lien entre noyau et image d'une AL}
	\subsection{Théorème 7 (Fondamental)}
	Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\sigma : V \rightarrow W$ est une AL,et si V est de dimension \textit{finie} $n$, alors :
	\begin{center}
		dim(ker $\sigma$) + dim(Im $\sigma$) = dim V
	\end{center}
	
	\section{Formes linéaires - Espace dual}
	\subsection{Définitions et propriétés}
	Si K, V, + est un espace vectoriel défini sur un corps K commutatif, alors l'ensemble des formes linéaires de V dans K est appelé l'\textit{espace dual} de V et est noté $V^{*}$.\\
	Autrement dit : $V^* = L(V, K)$.
	
	\subsection{Exemples de formes linéaires}
	\textit{Cf. cours}
	
	\subsection{Forme linéaire et noyau}
	\textbf{Théorème 1}\\
	Si K, V, + est un espace vectoriel défini sur un corps K commutatif et si $f : V \rightarrow K$ est une forme linéaire sur V, alors :
	\begin{center}
		Soit ker($f$) est un hyperplan de V, soit ker($f$) = V
	\end{center}
	\ \\
	\textbf{Défintion 3 :}\\
	Si K, V, + est un espace vectoriel défini sur un corps K commutatif. Une forme linéaire $f : V \rightarrow K$ est \textit{non dégénérée} $\Leftrightarrow$
	$$ker(f)\ est\ un\ hyperplan\ de\ V$$
	La forme linéaire sera donc \textit{dégénérée} $\Leftrightarrow ker(f) = V$.
	\subsection{Interprétation géométrique des forme linéaires}
	Ne fait pas partie de l'examen.
	
	\subsection{Expression d'une forme linéaire par rapport à une base de V}
	Si K, V, + est un espace vectoriel de dimension finie défini sur un corps K commutatif, alors toute forme linéaire $f : V \rightarrow L$ est \textit{univoquement déterminée} par les valeurs qu'elle prend sur les éléments d'une base de V.
	
	\subsection{Base de $K, V^*, +$ en dimension finie}
	Si K, V, + est un espace vectoriel de dimension finie $n$ défini sur un corps K commutatif, et muni d'une base $e = (\vec{e_1}, \vec{e_2}, ..., \vec{e_n})$ alors les $n$ applications :
	$$e_i^* : V \rightarrow K : \vec{x} = \sum_{j=1}^{n} x_j \vec{e_j} \rightarrow x_i = i^{ème}\ composante\ de\ \vec{x}\ dans\ la\ base\ e$$
	forment une base de $K, V^*, +$, appelée \textit{base duale de la base e} qui sera notée $e^* = (\vec{e^*_1}, \vec{e^*_2}, ..., \vec{e^*_n})$\\
	\ \\
	\textbf{Propriété 1 (essentielle)}\\
	Si K, V, + est un espace vectoriel de dimension finie $n$ défini sur un corps K commutatif, et muni d'une base $e = (\vec{e_1}, \vec{e_2}, ..., \vec{e_n})$ et si $e^* = (\vec{e^*_1}, \vec{e^*_2}, ..., \vec{e^*_n})$, alors :
	$$\forall i, j = 1, ..., n = e_i^*(\vec{e_j}) = \delta_{ij}$$
	(où $\delta_{ij}$ est le symbole de Kronecker)
	
	\subsection{Composante d'une forme linéaire dans une base de $K, V^*, +$}
	Partie essentiellement pratique (\textit{cf. TP}).
	
	
	\chapter{Rang de matrices - Systèmes d'équations linéaires}
	\section{Rang d'une matrice}
	Cette partie est fort \textit{recopiage} mais il n'y a pas vraiment le choix.
	\subsection{Rang d'un système de vecteurs et rang d'une matrice}
	\textbf{Définition 1} :\\
	Le \textit{rang d'un système de vecteurs} d'un espace vectoriel K, V, + est la dimension du sous-espace vectoriel de $V$, engendré par cet ensemble de vecteurs.\\
	\\
	\textbf{Définition 2} : \\
	Si $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif,alors le \textit{rang de A} est le nombre maximale de \textit{"colonnes de A linéairement indépendantes"}, c'est-à-dire de manière plus précise que le \textit{rang de A} est le rang du système de vecteurs-colonnes déterminé par les $n$ colonnes de $A$.\\
	\\
	\textit{NB :} Plus simplement (et moins mathématiquement du coup \ ;D), on peut considérer le le rang de A est le nombre de \textit{colonnes} linéairement indépendantes.
	
	\subsection{Détermination du rang d'une matrice A d'ordre (p,n)}
	C'est long et peu utile ici, regardez les slides ou elle donne un bon gros exemple bien détaillé !
	
	\subsection{Propriété des rangs de matrices}
	\textit{Si A est une matrice d'ordre $p \times n$} à coefficients dans un corps K commutatif, alors  :
	$$rang(A) \leq n$$
	\textit{Si A est une matrice d'ordre $p \times n$} à coefficients dans un corps K commutatif, alors :
	$$rang(A) = rang( ^t A)$$
	\textbf{Corollaire 1 :}\\
	i $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif,alors :
	$$rang(A) \leq min{p,n}$$
	C'est 'logique' dans le sens ou une matrice de trois colonnes ne peut avoir quatre (et plus ) vecteurs LI.\\
	\\
	\textbf{Corrolaire 2 :}\\
	Le rang d'une matrice $A$ ) coefficients dans un corps K commutatif peut se calculer aussi bien à partir d'un système de vecteurs-ligne associé à cette matrice qu'à partir du système de vecteurs-colonne associé.\\
	\\
	\textbf{Théorème :}\\
	Si $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif,alors :
	$$rang(A) = dim(Im \alpha)$$
	ou $\alpha : V \rightarrow W$ est une application linéaire, K, V, + est une EV de dimension $n$ et de base $e$, K, W, + est une EV de dimension $p$ et de base $\epsilon$ et la matrice $\alpha$ dans les bases $e$ et $\epsilon$ est la matrice donnée $A$.\\
	\\
	\textbf{Lien avec les déterminants :}
	Si $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif. Considérons l'ensemble $\lbrace A_i\ \vert\ i=1,...,min\{p,n\}\rbrace$ de toutes les sous-matrices carrées extraites de $A$. On peut alors démontrer que :
	$$rang(A) = max\ \lbrace\ ordre\ A_i\ \vert\ det(A_i) \neq 0 ; i = 1,...,min\{p,n\}\ \rbrace$$
	\\
	En français abusif, on peut dire que : '\textit{le rang de A est l'ordre du plus gros déterminant non nul que l'on peut extraire de la matrice A}'.\\\\
	\textit{NB :} C'est généralement une mauvaise idée de faire ça, il faut plus l'utiliser quand on est presque à la fin si ça peut faire gagner du temps.
	
	
	\chapter{Formes bilinéaires et produits scalaires}
	\section{Formes bilinéaires}
	\subsection{Définitions}
	Si V, + est un espace vectoriel réel de dimension finie, sur un corps K commutatif, on appelle \textbf{forme bilinéaire sur V} toute application 
	$$f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$$
	tel qu'elle est linéaire à gauche et à droite\\
	
	On dira qu'une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{symétrique} $\Leftrightarrow \forall \vec{x}, \vec{y} \in V : f(\vec{x}, \vec{y}) = f(\vec{y}, \vec{x})$.
	
	\subsection{Exemples de formes bilinéaires}
	\textbf{Produit scalaire usuel}\\
	Soit $V = \underline{\mathbb{R}^n}$, on appelle produit scalaire usuel l'application :
	$$< , > : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} = (\vec{x}, \vec{y}) \rightarrow <\vec{x}, \vec{y}> = \sum_{i=1}^n x_iy_i$$
	avec $\vec{x} = (x_1, x_2, ..., x_n) \in \mathbb{R}^n$ et $\vec{y} = (y_1, y_2, ..., y_n) \in \mathbb{R}^n$.
	
	\textbf{Attention :} il ne nécessite aucune base, les vecteurs sont des n-uples réels.\\
	
	\emph{Théorème} : 
	Le produit salaire usuel est une forme bilinéaire \textbf{symétrique} sur le corps des réels.
	
	\section{Espace vectoriel des formes bilinéaires}
	Soit \textit{Bil}(V) \textbf{l'ensemble des formes bilinéaires} sur un espace vectoriel K,V,+. \\
	Par conséquent :
	$f \in \mathcal{B}il(V)$ signifie que $f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$\\
	Si on munit $\mathcal{B}il(V)$ d'une addition vectorielle et d'une multiplication par un scalaire (\textit{def p. 161}) alors $K,\mathcal{B}il(V),+$ est un espace vectoriel.
	
	\subsection*{Théorème}
	Si V, + est un espace vectoriel de dimension finie n sur un corps K commutatif et si $e = (\vec{e_1}, ..., \vec{e_n})$ est une base de V, alors les $n^2$ formes bilinéaires 
	$$g_{ij} : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow g_{ij}(\vec{x}, \vec{y}) = e_i^*(\vec{x})e_j^*(\vec{y})\ (\forall i, j : 1,2,...n)$$
	forment une base de $K,\mathcal{B}il(V),+$.\\
	
	\emph{Corolaire}\\
	Soient V, + est un espace vectoriel de dimension finie n sur un corps K commutatif muni d'une base $e = (\vec{e_1}, ..., \vec{e_n})$ et $f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$ une forme bilinéaire définie sur V, alors
	$$f = \sum_{i=1}^n\sum_{j=1}^n f(\vec{e_i}\vec{e_j})g_{ij}$$
	et les composantes de $f$ dans la base $(j_{ij} | i,j= 1,2,...n)$ de \textit{Bil}(V) sont \textbf{les images par f des couples de vecteurs de la base e}.(\textit{Bon exemple page 165}).
	
	\section{Matrice d'une forme bilinéaire dans une base donnée}
	Pour aller droit au but, en considérant la base canonique $e = (\vec{e_1}, \vec{e_2}, \vec{e_3})$, \textbf{la matrice $F^e$ de la forme bilinéaire $f$ dans la base $e$} :
	$$\begin{pmatrix}
	f(\vec{e_1}, \vec{e_1}) & f(\vec{e_1}, \vec{e_2}) & f(\vec{e_1}, \vec{e_3})\\
	f(\vec{e_2}, \vec{e_1}) & f(\vec{e_2}, \vec{e_2}) & f(\vec{e_2}, \vec{e_3})\\
	f(\vec{e_3}, \vec{e_1}) & f(\vec{e_3}, \vec{e_2}) & f(\vec{e_3}, \vec{e_3})\\
	\end{pmatrix}$$
	Connaissant deux vecteurs $\vec{x}$ et $\vec{y}$, on peut calculer $f(\vec{x}, \vec{y})$ de deux façons différentes : en remplaçant directement dans l'expression ou de façon matricielle :$f(\vec{x}, \vec{y}) =\ ^tX_e F^e Y_e$.
	$$f(\vec{x}, \vec{y})\ =\ \left(x_1, x_2, x_3\right)\begin{pmatrix}
	f(\vec{e_1}, \vec{e_1}) & f(\vec{e_1}, \vec{e_2}) & f(\vec{e_1}, \vec{e_3})\\
	f(\vec{e_2}, \vec{e_1}) & f(\vec{e_2}, \vec{e_2}) & f(\vec{e_2}, \vec{e_3})\\
	f(\vec{e_3}, \vec{e_1}) & f(\vec{e_3}, \vec{e_2}) & f(\vec{e_3}, \vec{e_3})\\
	\end{pmatrix}\begin{pmatrix}
	y_1\\
	y_2\\
	y_3\\
	\end{pmatrix}$$
	
	\section{Formes bilinéaires et changements de bases}
	Le lien entre la matrice $F^e$ de $f$ dans la base $e$ et la matrice $F^u$ de $f$ dans la base $uu$ est :
	$$F^u =\ ^t(P_e^u)F^eP^u_e$$
	\emph{Définition}\\
	Le \textbf{rang d'une forme bilinéaire} $f : V \times V \rightarrow K$ est le rang de la matrice de $f$ dans une base quelconque de V.
	
	\section{Produits scalaires}
	\subsection{Définitions}
	\emph{Définition 1}\\
	Si V, + est un espace vectoriel \textbf{réel} on appelle \textbf{produit scalaire sur V} tout \textit{forme bilinéaire symétrique} sur V. On utilisera pour les produits scalaire la notation qui suit :
	$$< , > : V \times V \rightarrow \mathbb{R} = (\vec{x}, \vec{y}) \rightarrow <\vec{x}, \vec{y}>$$\\
	
	\emph{Définition 2}\\
	Un produit scalaire $< , >$ défini sur un espace vectoriel $\mathbb{R}, V, +$ est \textbf{défini positif} $\Leftrightarrow\  < , >$ est un produit scalaire tel que :
	\begin{enumerate}
		\item $\forall \vec{x} \in V : <\vec{x}, \vec{x}> \geq 0\ \ (positif)$
		\item $< \vec{x}, \vec{x}> = 0 \Leftrightarrow \vec{x} = \vec{0}$
	\end{enumerate}
	\subsection{Exemples de P.Scal (à connaître)}
	\begin{enumerate}
		\item Le produit scalaire \textbf{usuel} (voir plus haut)
		\item $< , > : \mathcal{P}_3 \times \mathcal{P}_3 \rightarrow \mathbb{R} : (\vec{p}, \vec{q}) \rightarrow <\vec{p}, \vec{q}> = \int_0^1 p(t)q(t) dt$
		\item $< , > : \mathbb{R}^{p \times n} \times \mathbb{R}^{p \times n} \rightarrow \mathbb{R} : (A, B) \rightarrow <A, B> = tr(A\ \ ^tB)$
	\end{enumerate}
	
	\section{Espaces euclidiens centrés}
	Un espace \textbf{euclidien centré} est un espace vectoriel \textbf{réel} muni d'un produit scalaire \textbf{défini positif} noté $\mathbb{R},V,+, < , >$.
	
	\section{Propriété de formes bilinéaires particulières}
	\textbf{Définition 1} : Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{symétrique} ssi $\forall \vec{x}, \vec{y} \in V = f(\vec{x}, \vec{y}) = f(\vec{y}, \vec{x})$.\\
	
	\textbf{Théorème 1} :
	Une forme bilinéaire $f : V \times V \rightarrow K$ est symétrique ssi il existe une base $e$ de $V$ dans laquelle la matrice de $f$ est symétrique.\\
	
	\textbf{Définition 2} : Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{antisymétrique} ssi $\forall \vec{x}, \vec{y} \in V = f(\vec{x}, \vec{y}) = -f(\vec{y}, \vec{x})$.\\
	
	\textbf{Définition 3} : Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{alternée} ssi $\forall \vec{x} \in V : f(\vec{x}, \vec{x}) = 0$.\\
	
	\textbf{Théorème 2} : Si $2 \neq 0$ dans K, alors tout forme bilinéaire antisymétrique est alternée.\\
	
	\textbf{Théorème 3} : Toute forme bilinéaire alternée est antisymétrique.\\
	
	\textbf{Définition 1} : Si $2 \neq 0$ dans K, alors toute forme bilinéaire $f : V \times V \rightarrow K$ est la somme d'une forme bilinéaire symétrique et d'une forme bilinéaire alternée.\\
	
	\section{Orthogonalité}
	\subsection*{Définition}
	Soit K, V, + un EV de dimension finie, défini sur un corps K commutatif et $f : V \times V \rightarrow K$ une forme bilinéaire sur $V$. On dira qu'un \textbf{vecteur} $\vec{x} \in V$ est \textbf{orthogonal} à $\vec{y} \in V\ \ (\vec{x} \perp \vec{y}) \Leftrightarrow$
	$$f(\vec{x}, \vec{y}) = 0$$
	Hélas, $\perp$ n'est pas toujours symétrique : si $f(\vec{x}, \vec{y}) 0 = 0 \neq f(\vec{y},\vec{x})$ on risque d'avoir un souci ! Sont-ils perpendiculaires ou non ? Pour régler le souci, il y a la \textit{réflexivité}.
	
	\subsection*{Définition}
	Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{réflexive} 
	$$\Leftrightarrow \forall \vec{x}, \vec{y} \in V \left[ \vec{x} \perp \vec{y} \Rightarrow \vec{y} \perp \vec{x}\right]$$
	Ainsi, une forme bilinéaire est réflexive $\Leftrightarrow$ \textbf{f est symétrique ou alternée}.\\
	
	Hélas, il peut exister des vecteurs non nuls de $V$ qui sont orthogonaux à eux mêmes.
	\subsection*{Définition}
	Soit K, V, + un EV de dimension finie, défini sur un corps K commutatif et $f : V \times V \rightarrow K$ une forme bilinéaire sur $V$. \\
	On appelle \textbf{vecteur isotrope de V}, tout vecteur $\vec{x} \in V$ tel que $f(\vec{x}, \vec{y}) = 0$.\\
	
	Pire, s'il existe au moins un vecteur non nul de $V$ qui est orthogonal à tous les vecteurs de $V$ on dira que $f$ est \textbf{dégénérée}. Dans le cas contraire (ou il n'y en a aucun) f est dite \textbf{non dégénérée}.
	
	\subsection*{Théorème}
	En passant le blabla de la base et de l'EV, si $F^e$ est la matrice d'une forme bilinéaire dans la base $e$ alors f est \textbf{dégénérée} 
	$$\Leftrightarrow det(F^e) = 0\ \ \Leftrightarrow rang\ de\ f\ < n$$
	
	
	\chapter{Produits hermitiens}
	\section{Produits hermitiens ou forme hermitiennes}
	Si $V, +$ est un espace vectoriel \underline{complexe} ($K = \mathbb{C}$) de dimension finie, on appelle \textbf{produit hermitien} ou \textbf{forme hermitienne sur V} tout application $h : V \times V \rightarrow \mathbb{C} : (\vec{x}, \vec{y}) \rightarrow h(\vec{x}, \vec{y})$ telle que $\forall \vec{x}, \vec{y}, \vec{z} \in V\ et\ \forall \lambda \in \mathbb{C} :$
	\begin{enumerate}
		\item $h(\vec{x} + \vec{y}, \vec{z}) = h(\vec{x}, \vec{z}) + h(\vec{y}, \vec{z})$
		\item $h(\lambda\vec{x}, \vec{z}) = \lambda h(\vec{x}, \vec{z})$
		\item $h(\vec{x}, \vec{z}) = \overline{h(\vec{z}, \vec{x})}$
	\end{enumerate}
	où $\overline{h(\vec{z}, \vec{x})}$ désigne le conjugué de $h(\vec{z}, \vec{x})$ dans $\mathbb{C}$.\\
	
	La condition \textit{3.} implique la \textbf{semi-linéarité à droite} de $h$:\\
	$\forall \vec{x}, \vec{y}, \vec{z} \in V\ et\ \forall \lambda \in \mathbb{C} :$
	\begin{enumerate}
		\item $h(\vec{x}, \vec{y} + \vec{z}) = h(\vec{x}, \vec{y}) + h(\vec{x}, \vec{z})$
		\item $h(\vec{x}, \lambda\vec{z}) = \overline{\lambda}h(\vec{x}, \vec{z})$
	\end{enumerate}
	Cette deuxième implication expliquera la présence du conjugué à droite dans l'écriture matricielle du produit hermitien. \\
	\textit{NB :} On général, on désigne les produits hermitiens par $< , >$ et non par $h$.
	
	\subsection*{Définition 2 (A connaître par keur-keur)}
	Soit $V = \mathbb{C}^n$, on appelle \textbf{produit hermitien usuel} l'application :
	$$< , > : \mathbb{C}^n  \times \mathbb{C}^n \rightarrow \mathbb{C} : (\vec{x}, \vec{y}) \rightarrow <\vec{x}, \vec{y}> = \sum_{i=1}^n x_i\overline{y_i}$$
	où $\vec{x}$ et $\vec{y}$ appartiennent à $\mathbb{C}$.
	
	\section{Matrice d'un produit hermitien dans une base donnée}
	C'est exactement la même chose que pour le produit scalaire si ce n'est qu'on utilise le produit hermitien.
	$$G^e = \left( <\vec{e_i}, \vec{e_j}>\right)$$
	
	On calcule ainsi le produit hermitien de la façon suivante : 
	$$<\vec{x}, \vec{y}> = \left(^t X_e\right)G^e\left(\overline{Y_e}\right)$$
	\textbf{Attention :} on conjugue les composantes de $\vec{y}$ !
	
	\subsection*{Théorème}
	On dira qu'une application dans une base $e$ est un produit hermitien sur $V$ ssi $H^e$ est un matrice hermétique ($H = ^t(\overline{A})$).
	
	\section{Matrice d'un produit hermitien et changement de base}
	Semblable aux produits scalaires, il ne faut juste pas oublier la conjuguée
	$$G^u =\ ^t(P^u_e) G^e (\overline{P^u_e})$$
	
	\section{Produits hermitiens définis positifs}
	Tatatitatata sera défini positif ssi :
	\begin{enumerate}
		\item $\forall \vec{x} \in V : <\vec{x}, \vec{x}> \in \mathbb{R}$
		\item $\forall \vec{x} \in V : \vec{x}, \vec{x}> \geq 0$
		\item $\forall \vec{x} \in V : \vec{x}, \vec{x}> = 0 \Leftrightarrow \vec{x} = \vec{0}$
	\end{enumerate}
	Par propriété, le produit hermitien usuel est défini positif.
	
	\section{Espaces hermitiens centrés}
	\textbf{Un espace hermitien centré} (ou \textbf{espace préhilbertien complexe}) est un espace vectoriel complexe muni d'un produit hermitien défini positif.
	
	
	\chapter{Orthogonalité et normes}
	\section{Espaces euclidiens et hermitiens centrés}
	Un espace \underline{euclidien centré} est un espace vectoriel réel muni d'un produit scalaire défini positif.\\
	Un espace \underline{hermitien centré} est un espace vectoriel complexe muni d'un produit hermitien défini positif.
	
	\section{Sous-espace orthogonaux}
	\subsection*{Définition 1}
	Si $V, < , >$ est un espace euclidien ou hermitien centré, alors deux vecteur $\vec{x}$ et $\vec{y}$ de $V$ sont \textbf{orthogonaux} \textit{pour} $< , >$ ssi $<\vec{x}, \vec{y}> = 0$.
	
	\subsection*{Définition 2}
	Si $S$ est un sous-ensemble non vide d'un espace euclidien ou hermitien centré $V, < , >$, alors 
	$$S^{\perp} =  \left\{\vec{y} \in V | <\vec{x}, \vec{y}> = 0\ \forall \vec{x} \in S \right\} = l'orthogonal\ de\ S\ pour\ <\ ,\ >$$
	
	\subsection*{Propriétés}
	Si $S$ est un sous-espace vectoriel d'un espace euclidien ou hermitien centré V, < , >, alors $S^{\perp}$ est aussi un sous-vectoriel de V, < , >. On appelle alors $S^{\perp}$ le \textbf{sous-espace orthogonal de} $S$.\\
	
	Si $S$ est un sous-espace vectoriel d'un espace euclidien ou hermitien centré V, <, >, alors 
	$$dim(S) + dim(S^\perp) = dim(V)$$.\\
	
	Si $S$ est un sous-ensemble non vide d'un espace euclidien ou hermitien centré V, <, > alors $S \cap S^\perp = \{\vec{0}\}\ si\ \vec{0} \in S\ ou\ \phi\ si\ \vec{0} \notin S$.\\
	
	Si $S = V$ alors $S^\perp = \{\vec{0}\}$. ; le vecteur nul est le seul vecteur d'un espace euclidien (ou hermitien) centré orthogonal à tous les vecteurs de l'espace.\\
	
	Si $V, <, >$ est un espace euclidien ou hermitien centré, alors tout sous-ensemble de vecteur \textbf{non nuls, orthogonaux deux à deux} forme une partie libre de $V$.
	
	\section{Projection orthogonale - Coefficient de Fourier}
	\subsection*{Définition 3}
	Si $V, < , >$ est un espace euclidien ou hermitien centré, et si $\vec{x}$ et $\vec{y}$ sont deux vecteurs \textit{non nuls} de V, le \textbf{coefficient de Fourrier de $\vec{x}$ par rapport à $\vec{y}$} vaut : 
	$$\lambda = \frac{\langle\vec{x}, \vec{y}\rangle}{\langle\vec{y}, \vec{y}\rangle}$$
	
	Si $V, \langle , \rangle$ est un espace euclidien ou hermitien centré, et si $\vec{x}$ et $\vec{y}$ sont deux vecteurs \textit{non nuls} de V,la \textbf{projection orthogonale de $\vec{x}$ su $\vec{y}$} est le \textsc{vecteur} :
	$$\vec{p} = \frac{\langle\vec{x}, \vec{y}\rangle}{\langle\vec{y}, \vec{y}\rangle}\vec{y}$$
	
	\subsection{Théorème important :}
	Si V, <, > est un espace euclidien ou hermitien centré, et si $W$ est un sous-espace de dimension $k$ de $V$, $W$ étant muni d'une base $(\vec{e_1'}, ..., \vec{e_k'})$ orthogonale pour $\langle , \rangle$ alors \textbf{la projection orthogonale d'un vecteur $\vec{x} \in V - W$ sur $W$} est égale au vecteur 
	$$\vec{p} = \sum_{i=1}^k \frac{\langle\vec{x}, \vec{e_i'}\rangle}{\langle \vec{e_i'}, \vec{e_i'}\rangle}\vec{e_i'}$$
	Ce vecteur est la somme des projections orthogonales du vecteur $\vec{x}$ sur chacun des vecteurs de la base $e'$.\\
	
	De façon plus générale, si $\vec{x} = \sum_{i=1}^n x_i\vec{e_i}$, alors : ( où $x_j = \frac{\langle\vec{x}, \vec{e_j} \rangle}{\langle\vec{e_j}, \vec{e_j}\rangle}$) :
	$$X_e = \begin{pmatrix} 
	x_1\\
	\vdots\\ 
	x_j\\
	\vdots\\
	x_n \end{pmatrix}$$
	\textbf{Voir conclusion page 211}.
	
	\subsection*{Définition 5}
	Soit $\mathbb{R}, V, +, \langle , \rangle$ un espace euclidien ou hermitien centré de dimension finie $n$. Une \textbf{base} $e = (\vec{e_1}, ... \vec{e_n})$ est \textbf{orthonormée pour le produit $\langle , \rangle$}
	$$\Leftrightarrow \langle\vec{e_i}, \vec{e_j}\rangle = \delta_{ij}$$
	
	\section{Expression d'un produit scalaire dans une base orthonormée}
	\subsection{Théorème (très très important)}
	Si $K , V, +, \langle , \rangle$ est un espace euclidien ou hermitien centré de dimension finie $n$, muni d'une base $e$ \textbf{orthonormée pour le produit $\langle , \rangle$} alors 
	$$\forall \vec{x}, \vec{y} \in V : \langle \vec{x}, \vec{y}\rangle = \sum_{i=1}^n x_i\overline{y_i}$$
	où $X_e$ est la matrice des composantes de $\vec{x}$ dans la base orthonormée $e$ et $Y_e$ est la matrice des composantes de $\vec{y}$ dans la base orthonormée $e$.\\
	
	En français : Certains produits scalaires peuvent être assez contraignants à calculer. Mais, si l'on construit une base orthonormée pour ce \textit{moche} produit, en travaillant dans cette nouvelle base le \textit{moche} produit \textbf{se ramène au produit scalaire ou hermitien usuel à partir des composantes de ces vecteurs dans la base orthonormée trouvée} ce qui simplifie les calculs ! Houra, me direz-vous.
	
	
	\subsection*{Application importante : Théorème}
	$$\forall \vec{x}, \vec{y} \in V : \pscal{\alpha(\vec{x})}{\vec{y}} = \pscal{\vec{x}}{\alpha(\vec{x})}$$
	Si ce théorème est vérifié, on dira que $\alpha$ est \textbf{auto-adjoint} (ou hermitien).
	
	\section{Algorithme d'orthogonalisation de Gram - Schmidt}
	Partie essentiellement pratique, \textit{cf. TP}.
	
	\section{Norme euclidienne ou hermitienne}
	Si $V, \langle , \rangle$ est un espace euclidien (ou hermitien) centré, on appelle \textbf{norme euclidienne} (... hermitienne) \textbf{associée à} $\langle , \rangle$ toute application : 
	$$||\ ||  : V \rightarrow \mathbb{R}^+ : \vec{x} \rightarrow ||\vec{x}|| = \sqrt{\pscal{\vec{x}}{\vec{x}}}$$
	
	\subsection{Propriété de la norme euclidienne (ou hermitienne)}
	C'est relativement simple, \textit{cf. page 221 - 227}.
	
	\section{Norme généralisée}
	Si $K, V, +$ est un espace vectoriel réel ou complexe, on appelle \textbf{borme de $V$} toute application $||\ || : V \rightarrow \mathbb{R}^+ : \vec{x} \rightarrow ||\vec{x}||$ telle que : 
	\begin{enumerate}
		\item $\forall \vec{x} \in V : ||\vec{x}|| \geq 0$ et $\left[||\vec{x}|| = 0 \Leftrightarrow \vec{x} = \vec{0}\right]$
		\item $\forall \vec{x} \in V$ et $\forall \lambda \in K : ||\lambda\vec{x}|| = |\lambda |\ ||\vec{x}||$
		\item $\forall \vec{x}, \vec{y} \in V : ||\vec{x} + \vec{y}|| \leq ||\vec{x}|| + ||\vec{y}||$\ \ (inégalité triangulaire)
	\end{enumerate}
	
	\chapter{Formes quadratiques}
	A partir d'ici, cette synthèse comprend seulement ce qui à une application aux TP, cela ne sert à rien de couvrir plus (Cf. la prof). En gros la matière théorique de l'examen de juin porte sur la matière des TP.
	
	\section{Définition - forme polaire}
	Si V, + est un espace vectoriel de dimension finie sur un corps K commutatif, tel que $0 \neq 2$, et si $f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$ est une forme bilinéaire \textbf{symétrique}, alors on appelle \textbf{forme quadratique sur V associée à} $f$ l'application $q : V \rightarrow K : \vec{x} \rightarrow q(\vec{x}) = f(\vec{x}, \vec{x})$.\\
	
	Ainsi, toute forme bilinéaire symétrique définit une forme quadratique et réciproquement, la donnée d'une forme quadratique permet de reconstituer univoquement la forme bilinéaire associée.
	
	\section{Formes quadratiques réelles}
	Il existe toujours une base $u$ de V telle que $Q^u = F^u$ est une matrice \textbf{diagonale}.
	
	\section{Réduction de Gauss}
	Voir séance de TP, mais il est très important d'appliquer l'algorithme \textsc{à la lettre} !\\
	Celle-ci permet de trouver une base dans laquelle la forme quadratique est diagonale.
	
	\section{Loi d'inertie - Signature d'une forme quadratique}
	Toute représentation en somme de carrés d'une forme quadratique réelle aura 
	\begin{enumerate}
		\item Un nombre constant $P_q$ de termes à coefficients strictement positifs
		\item Un nombre constant $N_q$ de termes à coefficients strictement négatif
	\end{enumerate}
	La \textbf{signature de la forme quadratique q} est le couple $(P_q, N_q)$.\\
	
	Le nombre de termes non nuls d'une représentation diagonale d'une forme quadratique réelle $q$ s'appelle le \textbf{rang de q}, qui est égal au rang de la matrice de $q$ dans n'importe quelle base de l'espace.
	
	
	\chapter{Valeurs propres et sous-espaces propre d'un opérateur linéaire}
	\section{Définitions}
	Si V, + est un espace vectoriel sur un corps K commutatif, et si $\alpha : V \rightarrow V$ est un opérateur linéaire de V, alors on appelle \textbf{vecteur propre de $\alpha$} tout vecteur $\vec{x}$ de V tel que $\exists \lambda \in K\ |\ \alpha(\vec{x}) = \lambda\vec{x}$.\\
	
	\textbf{Définition 2}\\
	Si V, + est un espace vectoriel sur un corps K commutatif, et si $\alpha : V \rightarrow V$ est un opérateur linéaire de V, alors un \textbf{scalaire} $\lambda \in K$ est une \textbf{valeur propre de $\alpha$} ssi il \textbf{existe} un vecteur $\vec{x} \in V$ \textbf{non-nul} tel que $\alpha(\vec{x}) = \lambda \vec{x}$.\\
	
	\textbf{Définition 3}\\
	Le \textbf{sous espace propre de $\alpha$ associé à la valeur propre $\lambda$} est l'ensemble de tous les vecteurs propres de $\alpha$ associé à la valeur propre $\lambda$
	$$W_\lambda = \left\lbrace \vec{x} \in V\ |\ \alpha(\vec{x}) = \lambda\vec{x}\right\rbrace$$\\
	
	\textbf{Définition 4}\\
	L'ensemble des valeurs propres de $\alpha$ s'appelle le \textbf{spectre de l'opérateur $\alpha$}.
	
	\section{Valeurs propres particulières}
	\begin{itemize}
		\item[$\lambda = 1$] L'ensemble des valeurs propre de $\alpha$ associée à $\lambda = 1$ est l'ensemble des \textit{points fixes de $\alpha$}.
		\item[$\lambda = 0$] L'ensemble des valeurs propre de $\alpha$ associée à $\lambda = 0$ est \textit{le noyau $ker(\alpha)$ de $\alpha$}.
		\item[$\lambda = 0$] L'application $\alpha$ n'est pas injective (il faut en effet qu'il existe un vecteur non nul, ce qui n'est pas le cas ici)
	\end{itemize}
	
	
	%%%%%%%%%%%%%%%%%
	% Bibliographie %
	%%%%%%%%%%%%%%%%%
	%\newpage
	%\chapter{Bibliographie}
	%\nocite{*}
	%\printbibliography[heading=none]
	
	%%%%%%%%%%%
	% Annexes %
	%%%%%%%%%%%
	\appendix
	%\input{annexes/annexe1.tex}
\end{document}

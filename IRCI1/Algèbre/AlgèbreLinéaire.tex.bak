\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[french]{babel}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[left=3cm,right=3cm,top=2.7cm,bottom=3cm]{geometry}
\usepackage{url}
\usepackage{ulem}

\usepackage{hyperref} \hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black, urlcolor=black,}

\usepackage[toc]{multitoc}

\setcounter{tocdepth}{2}

\author{Nicolas \bsc{Englebert}}
\title{Synthèse du cours Algèbre linéaire}
\date{Février 2014}



\begin{document}
\renewcommand{\proofname}{Démonstration}
\newcommand{\pscal}[2]{\left\langle {#1} , {#2} \right\rangle} 
\maketitle
\tableofcontents
\newpage

\section{Structure}
\subsubsection{Relation}
Etant donné deux ensemble A et B, on appelle \textit{relation} de A vers B tout ensemble de couple dont l'origine appartient à A, et l'extrêmité à B :
$$ \forall a, b \in f \subseteq A \times B : b =f(a) = l'image\ de\ A\ par\ f$$
\textbf{Vocabulaire}\\
\begin{description}
\item[Application] Relation $A \longrightarrow B$ tel que A a pour image un seul élément de B par cette application
\item[Injection] $\forall x,y \in A : [x \neq y \Rightarrow f(x) \neq f(y)]$
\item[Surjection] $\forall b \in B, \exists a \in A : b = f(a)$
\item[Bijection] $\forall b \in B, \exists a! \in A : b = f(a)$
\end{description}
On appelle relation d'équivalence, toute relation :
\begin{itemize}
\item[Réflexive] $\forall a \in E : a \Re a$
\item[Symétrique] $\forall a, b \in E, a \Re b = b \Re a$
\item[Transitive] $\forall a, b, c \in E, a \Re b\ et\ b \Re c \Rightarrow a \Re c$
\end{itemize}
\ \ \\
On appelle relation d'ordre, toute relation :
\begin{itemize}
\item[Réflexive] $\forall a \in E : a \Re a$
\item[antisymétrique] $\forall a, b \in E, a \Re b\ et\ b \Re a \Rightarrow a = b$
\item[Transitive] $\forall a, b, c \in E, a \Re b\ et\ b \Re c \Rightarrow a \Re c$
\end{itemize}\ \\
On dira qu'une relation d'ordre $\wp$ dans un ensemble E forme un ordre total ssi $\forall a, b \in E, a \wp b\ ou b \wp a$\\
\textbf{Attention :} Il n'y a pas d'ordre total dans $\mathbb{C}$.


\subsection{Groupe}
\subsubsection{Définition générale}
Etant donné un ensemble E, une loi $\clubsuit$ sur E est une application :
$$\clubsuit: E \times E \rightarrow E : (x,y) \rightarrow x\ \clubsuit\ y$$
Pour être un groupe, il faut vérifier les 4 propriétés suivantes : 
\begin{itemize}
\item[$\clubsuit$ est \textit{interne} dans E] \ \ \ \ \ \ \ \ \ \  \ $\forall x, y \in E : x\ \clubsuit\ y \in E$
\item[$\clubsuit$ est \textit{associative} dans E]\ \ \ \ \ \ $\forall x, y, z \in E : (x\ \clubsuit\ y)\clubsuit\ z =  x\ \clubsuit(y\ \clubsuit\ z)$
\item[$\exists$ un \textit{neutre} pour $\clubsuit$ dans E] \ \ \  $\exists n \in E\ |\ \forall x \in E : x\ \clubsuit\ n = x = n\ \clubsuit\ x$
\item[$\clubsuit$ est \textit{symétrisable} dans E]\ \ \ \ \ $\forall x \in E\ |\ \exists x' \in E : x\ \clubsuit\ x' = n = x'\ \clubsuit\ x$
\end{itemize}
\ \\
On dira qu'un ensemble E muni d'une loi $\clubsuit$ est un groupe \textit{commutatif} (ou \textit{abélien}) ssi :
$$E, \clubsuit\ est\ un\ groupe\ et\ [\forall x, y \in E, x\ \clubsuit\ y = y\ \clubsuit\ x$$
\ \\
Un groupe est \textit{d'ordre $n$} ssi $|E| = \#E = n$.\\
Un élément $a$ d'un ensemble E muni d'une loi $\clubsuit$ est un \textit{absorbant pour $\clubsuit$} dans E ssi :
$$a \in E\ et\ [\forall x, y \in E : x\ \clubsuit\ a = a = a\ \clubsuit\ x]$$
\textbf{Attention :} Il y a uniticité du neutre et du symétrique (de chaque élément) pour une loi donnée dans un groupe.\\ \\
Une autre propriété importante est la \textit{simplifiabilité} (préciser le côté !) :
$$\forall a, b, c \in E : (a\ \clubsuit\ c) = (b\ \clubsuit\ c) \Leftrightarrow a = b\ (Simplifiabilité\ à\ droite)$$
$$\forall a, b, c \in E : (c\ \clubsuit\ a) = (c\ \clubsuit\ b) \Leftrightarrow a = b\ (Simplifiabilité\ à\ gauche)$$

\subsubsection{Isomorphisme de groupe}
Deux groupes E, $\clubsuit$ et G, $\bigstar$ sont \textit{isomorphes} $\Leftrightarrow$ il existe une bijection $\delta : E \rightarrow G$ telle que : 
$$\forall a, b \in E : \delta (a\ \clubsuit\ b) = \delta(a)\ \bigstar\ \delta(b)$$
On dira alors que la bijection $\delta$ est un isomorphisme entre les groupes E et G.\\
\textit{Un isomorphisme de groupe est une bijection conservant la structure du groupe}

\subsubsection{Sous-groupes d'un groupe}
Soit E, $\clubsuit$ un groupe. H, $\clubsuit$ est un \textit{sous-groupe} de  E, $\clubsuit$ ssi :
\begin{itemize}
\item[1] H est un \textit{sous-ensemble} de E
\item[2] $\forall x, y \in H : x\ \clubsuit\ y\ \in H$
\item[3] Le \textit{neutre} de E pour $\clubsuit \in E$
\item[4] $\forall x \in H$ : le symétrique de $x$ pour $\clubsuit$ dans E est un élément de H
\end{itemize}
\ \\
Un petit théorème en passant : \textit{Théorème de Lagrange} : Si H, $\clubsuit$ est un sous-groupe \textbf{fini} de E, $\clubsuit$ alors l'ordre de H divise l'ordre de E.

\subsection{Anneau}
Soit A, un ensemble munis de deux lois $\clubsuit$ et $\bigstar$.\\
A, $\clubsuit$, $\bigstar$ est un anneau ssi :
\begin{itemize}
\item[1] A, $\clubsuit$ est un groupe \textit{commutatif}
\item[2] $\bigstar$ est \textit{interne} et \textit{associatif} dans A
\item[3] $\bigstar$ distribue $\clubsuit$ dans A
\end{itemize}
\ \\
On dira que A, $\clubsuit$, $\bigstar$ est un anneau \textit{unital} ssi A, $\clubsuit$, $\bigstar$ est un anneau et s'il existe un \textit{neutre pour $\bigstar$} différent du neutre pour $\clubsuit$ dans A.

\subsection{Corps - champ}
Soit K, $\clubsuit$, $\bigstar$ un corps ssi : 
\begin{itemize}
\item[1] K, $\clubsuit$ est \textit{interne} et \textit{associatif} dans A
\item[2] $K_{n}$, $bigstar$ est un groupe \textit{commutatif} (ou $n$ est neutre pour $\clubsuit$ dans K)

\item[3] $\bigstar$ distribue $\clubsuit$ dans A
\end{itemize}
\ \\

Soit K, un ensemble muni de deux lois $\clubsuit$ et  $\bigstar$.\\
K, $\clubsuit$, $\bigstar$ un corps commutatif ou \textit{champ} ssi :
\begin{itemize}
\item[1] K, $\clubsuit$, $\bigstar$ est un corps
\item[2] $\bigstar$ est \textit{commutative} dans K
\end{itemize}

\newpage
\section{Espaces Vectoriels}
\subsection{Définition}
A INCLURE.

\subsection{Exemples d'EV}
\textit{Cf. cours}

\subsection{Prorpiétés des espaces vectoriels}
Si K, V, + est un espace vectoriel, alors :
\begin{enumerate}
\item $\forall x \in V : 0.\vec{x} = \vec{0}$
\item $\forall \lambda \in K : \lambda.\vec{0} = \vec{0}$
\item $\forall \lambda \in K, \forall \vec{x} \in V : \lambda \vec{0} \Rightarrow \lambda = 0\ ou\ \vec{x} = \vec{0}$
\item $\forall \lambda \in K, \forall \vec{x} \in V : \lambda(- \vec{x}) = -\lambda\vec{x}$
\item $\forall \lambda_{i} \in K, \forall \vec{x} \in V : (\sum_{1}^{i} \lambda_{i})\vec{x} = \sum_{1}^{i}(\lambda_{i}\vec{x})$
\item $\forall \lambda_{i} \in K, \forall \vec{x} \in V : (\sum_{1}^{i} \vec{x}_{i})\lambda = \sum_{1}^{i}(\lambda\vec{x_{i}})$
\end{enumerate}

\subsection{Sous-espaces vectoriels}
Soit W, un sous-ensemble d'un espace vectoriel K, V, + défini sur un corps K\\
K, W, + est un \textit{sous-espace vectoriel} ssi :
\begin{enumerate}
\item W est non vide (signifie $\vec{0} \in W$)
\item $\forall \vec{x}, \vec{y} \in W = \vec{x} + \vec{y} \in W$
\item $\forall \vec{x} \in W, \forall \lambda \in K : \lambda\vec{x} \in W$
\end{enumerate}
\textbf{Attention :} Les lois et corps doivent être identiques !\\
\textit{NB :} Un espace vectoriel est le plus grand sous-vectoriel de lui-même. De même K, $\{\vec{0}\}$, + est le plus petit des EV. Ces deux SV sont dit \textit{triviaux}.\\
\textit{NB.2 } L'intersection de deux sous-espaces vectoriel et un SEV.

\subsubsection{Lien avec les équations linéaire homogènes}
L'ensemble des solution d'une équation linéaire homogène à $n$ inconnues et à coefficient dans un corps K est un sous-espace vectoriel de $K, K^{n}, +$.

\subsubsection{Lien avec les systèmes d'équations linéaires homogènes}
L'ensemble des solution d'un système d'équations linéaires homogènes est un sous-espace vectoriel de $K^{n}$ défini sur un corps K (commutatif).

\subsection{Somme de SEV}
Soit K, V, + un espace vectoriel et $W_{1}, W_{2}$ deux SEV de V. On appelle \textit{somme de $W_{1}, W_{2}$} l'ensemble défini par :
$$W_{1}, W_{2} = \{\vec{w_{1}} + \vec{w_{2}}\ |\ \vec{w_{1}} \in W_{1}, \vec{w_{2}} \in W_{2}\}$$

Soit V, + un  EV sur un corps K et $W_{1}, W_{2}$ deux SEV de V. On dira que V est la somme directe (notée $\bigoplus$) de $W_{1}, W_{2}$ ssi
$$[V = W_{1} + W_{2}\ et\ W_{1} \cap W_{2} = \{\vec{0}\}]$$

\subsection{Isomorphisme d'espaces vectoriels}
Soient K, V, + et K, W, + deux EV défini sur le \textit{même corps}.\\
V est \textit{isomorphe} ) W (V $\cong $ W) ssi il existe une bijection $\sigma V \rightarrow W$ telle que : 
\begin{enumerate}
\item $\forall \vec{x}, \vec{y} \in V : \sigma(\vec{x} + \vec{y}) = \sigma(\vec{x}) + \sigma(\vec{y})$
\item $\forall \vec{x} \in V, \forall \lambda \in K : \sigma(\lambda\vec{x}) = \lambda\sigma(\vec{x})$
\end{enumerate}
\textbf{Attention :} Il s'agit d'une question typique de la partie théorique de l'examen de janvier.

\subsection{Parties génératrices}
Si K, V, + est un espace vectoriel et si P est un sous-ensemble de V, alors :\\
\textit{L(P)} est le sous-espace de V, engendré par P.\\\\
\textit{NB :} Toute partie contenant une partie génératrice est une partie génératrice. Une partie génératrice est dite minimale, s'il n'existe pas de sous ensemble inclus dans P qui soit également génératrice.

\subsection{Combinaisons linéaires de vecteurs}
Si X est une partie d'un espace vectoriel K, V, +, on appelle \textit{combinaison linéaire des vecteurs de X} ou \textit{combili des vecteurs de X} tout vecteur V de la forme :
$$\lambda_{1}\vec{x_{1}} + \lambda_{2}\vec{x_{2}} + ... + \lambda_{n}\vec{x_{n}}$$
où les $\vec{x_{i}}$ sont les éléments de X \textbf{en nombre fini} et $\lambda_{i}$ sont des élément du corps K.\\\\
\textit{Théorème :} Pour toute partie non vide X d'un espace vectoriel K, V, + le sous-espace L(X) engendré par X est l'ensemble des combili des vecteurs de X.

\subsection{Parties libres de vecteurs}
\textit{Voir horrible définition dans le cours (\textbf{Attention :} AVC possible)}.\\
Notons tout de même que si X est un sous-ensemble d'un EV K, V, + contenant $\vec{0}$ alors X \textbf{n'est pas} une partie libre de V.\\
\textit{NB :} Si X est une partie libre d'un EV, alors tout sous-ensemble de X est une partie libre de V (L'ensemble vide également)\\\\
\textbf{Théorème } : Soit X, une partie libre d'un EV K, V, +.\\
X est une PL ssi :
$$\forall \vec{x_{i}} \in X, \forall \lambda_{i} \in K$$
$$[\sum_{i=1}^{n} \lambda_{i}\vec{x_{i}} = \vec{0} \Rightarrow \lambda_{1}, \lambda_{2}, ... \lambda_{n} = 0]$$
Deux vecteurs sont LI s'ils forment une PL de V.

\subsection{Bases et dimension}
Une partie sera une base $\Leftrightarrow$ celle-ci est à la fois libre et génératrice. On dira qu'une base est une \textit{partie libre minimale} et une \textit{partie génératrice maximale}.\\\\
\textbf{Théorème :} Si K, V, + est un espace vectoriel, si L est une PL de V et si G est une PG de V contenant L, alors il existe une base B de V telle que $L \subseteq B \subseteq G$.\\
\textit{Ce théorème permet d'étendre une partie libre pour former une base}.\\\\
Notons également :
\begin{itemize}
\item Tout EV possède une base.
\item S'il existe une base finie de n élément, toute base comporte n élément.
\item Les bases d'un même EV ont le même cardinal.
\item Toute PL a au plus n éléments.
\item Toute PG a au max n éléments.
\end{itemize}
\ \\
Si la dimension d'un EV est finie et vaut n, alors :
\begin{itemize}
\item Toute partie libre de n éléments est une base de cet EV.
\item Toute partie génératrice de n éléments est une base de cet EV.
\end{itemize}
\ \\
\textit{NB :} $dim(W_{1} + W_{2}) = dim(W_{1}) + dim(W_{2}) - dim(W_{1} \cap W_{2})$\\\\
Si B est une base d'un espace vectoriel, alors tout vecteur de V s'exprime \textbf{d'une et une seule manière} comme combili d'un nombre \textbf{fini} des vecteurs de B.

\subsection{Base canonique}
Liste de bases à connaître par coeur ! \textit{Cf. cours}

\subsection{Écriture matricielle d'une vecteur dans une base donnée}
$\forall x \in V : \exists 1!(\lambda_{1}, \lambda_{2}, ..., \lambda_{n}) \in K^{n}\ |\ \vec{x} = \sum_{i=1}^{n} \lambda_{i}\vec{x_{i}}$, les vecteurs $\vec{e_{i}}$ étant vecteurs de B.\\
\textbf{Convention de notation :}\\
$$\vec{x} = \sum_{i=1}^{n}  \lambda_{i}\vec{e_{i}} = \lambda_{1}\vec{e_{1}}, \lambda_{2}\vec{e_{2}}, ..., \lambda_{n}\vec{e_{n}} = (\vec{e_{1}}\ \vec{e_{2}}\ \ \ \ \vec{e_{n}})\begin{pmatrix} 
\lambda_1 \\ 
\lambda_2 \\ 
\vdots\\ 
\lambda_2 \end{pmatrix}$$
On nommera $\begin{pmatrix} 
\lambda_1 \\ 
\lambda_2 \\ 
\vdots\\ 
\lambda_2 \end{pmatrix}$ \textit{matrice des composantes de $\vec{x}$ dans la base B. Notation : $X_B$}.
\\
\textbf{Attention :} Ne pas convondre un vecteur et sa matrice de 
composantes.


\subsection{Changement de bases et de composantes}
(\textit{On suppose K, V, + est un EV de dimension finie sur un corps K commutatif})\\
\subsubsection{Matrice de changement de base de $e$ vers $\epsilon$}
Cette section (et les deux suivantes) étant principalement pratique, je ne me contenterai ici que de reprendre les notations (\textit{cf. TP5/6} :) )\\
$P_e^a$ se lit :
\begin{itemize}
\item Matrice de changement de base de $e$ vers $a$ (On "monte" dans les \textbf{B}ase comme \textbf{B}uzz l'éclair).
\item Matrice de changement de composante de $a$ vers $e$ (On "descend", on \textbf{C}reuse les \textbf{C}omposantes).
\end{itemize}
\subsubsection{Détermination de la patrice $P_\epsilon^e$}
$$P_\epsilon^e = (P_e^\epsilon)^{-1}$$

\subsubsection{Détermination de $X_\epsilon$ des vecteurs de $\vec{x}$ dans la base $\epsilon$ à partir de la matrice $X_e$ des composantes de $\vec{x}$ dans la base $e$}
$$X_\epsilon = P_\epsilon^e X_e$$
où $P_\epsilon^e$ est la matrice de changement de base de $\epsilon$ vers $e$.

\subsection{Droites, plans et hyperplans vectoriels}
\subsubsection{Droite vectorielle}
Si K, V, + est un espace vectoriel de dimension au moins 1, alors on appelle une \textit{droite (vectorielle)} de V tout SEV de V de dimension 1.

\subsubsection{Plan vectoriel}
Si K, V, + est un espace vectoriel de dimension au moins 2, alors on appelle une \textit{plan (vectoriel)} de V tout SEV de V de dimension 2.

\subsubsection{Hyperplan vectoriel}
Soit K, V, + un espace vectoriel. Un sous-espace vectoriel est un \textit{hyperplan (vectoriel)} de V $\Leftrightarrow$ H est un sous-espace vectoriel propre et maximal de V.\\
\begin{itemize}
\item[Maximal] : $\Leftrightarrow \nexists$ de SEV S de V tel que $H\subset S \subset V$
\item[Propre] : $H \neq V$
\end{itemize}
\ \\
\textit{La fin du chapitre est à lire à titre informatif}.

\newpage
\section{Applications linéaires}
\subsection{Définition}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un même corps K, on appelle \textit{application linéaire} de V dans W tout application $\sigma : V \rightarrow W$ telle que :
$$\forall \vec{x}, \vec{y} \in V : \sigma(\vec{x} + \vec{y}) = \sigma(\vec{x}) + \sigma(\vec{y})$$
$$\forall \lambda \in K, \forall \vec{x} \in V : \sigma(\lambda\vec{x}) = \lambda \sigma(\vec{x})$$
\textbf{Cas particuliers importants :} 
\begin{enumerate}
\item Si $\sigma$ est une application linéaires de V dans W, avec V=W, alors $\sigma$ es appelé \textit{opérateur linéaire} ou \textit{endomorphisme} de V.
\item Si $\sigma$ est une application de V dans W, avec W=K, alors $\sigma$ est une \textit{forme linéaire} ou \textit{covecteur} définie sur V.
\item Si $\sigma$ est une bijection linéaire de V dans W, alors $\sigma$ est un \textit{isomorphisme} de V sur W.
\item Si $\sigma$ est un opérateur linéaire de V \textit{et} une bijection, alors $\sigma$ est un \textit{automorphisme} de V.
\end{enumerate}

\subsection{Matrices d'une AL dans les bases données}

Encore une fois, c'est principalement pratique (\textit{cf TP6}). Néanmoins : \\
$A_\epsilon^e$, matrice de $\alpha$ dans les bases $e$ et $\epsilon$, est une matrice de p ligne(cardinal de la base $\epsilon$ et de n colonnes (cardinal de la base $e$).\\\\
\textbf{Attention :} Les matrices de rotations sont à étudier par coeur! Ne pas oublier de donner les bases, sinon cela n'a pas de sens ! 

\subsection{Composante de l'image d'un vecteur par une AL}
Même remarque qu'au point précédent, partie essentiellement pratique.

\subsection{Composée de deux AL}
Si K, V, +, K, W, + et K, S, + sont trois espaces vectoriels définis sur un même corps K et si $\alpha : V \rightarrow W$ et $\beta : W \rightarrow S$ sont deux applications linéaires, alors $\beta  \circ  \alpha$ est une application linéaire.

\subsection{Matrices d'AL et changement de base}
Partie pratique. (\textit{Cf. TP6})

\subsection{Espace vectoriel des AL}
\subsubsection{Théorème 1}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un coprs K \textit{commutatif} et si $L(V, W)$ est l'ensemble de toutes les applications linéaire $V \rightarrow W$, alors $K, L(V,W), +$ est un espace vectoriel sur K.\\\\
\textbf{Attention :} Regarder attentivement la définition de l'addition vectorielle et de la multiplication par un scalaire.

\subsubsection{Théorème 4}
Si K, V, + est un espace vectoriel de dimension \textit{finie} $n$, défini sur un corps K \textit{commutatif}, et si K, W, + est un espace vectoriel de dimension \textit{finie} p défini sur le même corps K, alors :
$$K, L(V, W), + \cong K, K^{p \times n}, +$$

\textbf{Corolaire}
$$dim(L(V,W)) = pn$$

\subsection{Noyeau d'une AL}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K \textit{commutatif}, et si $\alpha : V \rightarrow W$ est une application linéaire, alors le \textbf{noyau de $\alpha$}, noté \textit{ker $\alpha$}, est l'ensemble :
$$ker\ \alpha = \{\vec{x} \in V\ |\ \alpha(\vec{x}) = \vec{0}\ (de\ W)\}$$

\subsubsection{Théorème 1}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K commutatif et si $\alpha V \rightarrow W$ est une application linéaire, alors le vecteur nul de V appartient toujours au noyau de $\alpha$.

\subsubsection{Théorème 2}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K commutatif, et si $\alpha : V \rightarrow W$ est une AL, alors :
$$\alpha\ est\ injective\ \Leftrightarrow ker\ \alpha = \{\vec{0}\}$$

\subsubsection{Théorème 3}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors K, $ker\ \alpha$, + est un sous-espace vectoriel de K, V, +

\subsubsection{Autre définition}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors on appelle \textit{nullité de $\alpha$} la dimension de $\ker\ \alpha$.

\subsection{Image d'une AL}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K \textit{commutatif},  et si $\alpha : V \rightarrow W$ est une AL, alors on appelle \textit{image de $\alpha$}, noté $Im\ \alpha$ l'ensemble :
$$Im\ \alpha = \{\vec{y} \in W\ |\ \exists \vec{x} \in V\ avec\ \alpha(\vec{x}) = \vec{y}\}$$

\subsubsection{Théorème 4}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL alors :
$$\alpha\ est\ surjective\ \Leftrightarrow\ Im\ \alpha = W$$

\textbf{Corolaire :}\\
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL alors $\alpha$ est \textit{bijective} $\Leftrightarrow$
$$ker\ \alpha = \{\vec{0}\}\ et\ Im\ \alpha = W$$

\subsubsection{Théorème 5}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K, alors l'image d'un sous-espace vectoriel $S$ de $V$ par une AL$\alpha : V \rightarrow W$ est un sous-espace vectoriel de $W$.
\\\\
\textbf{Corolaire :}\\
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors K, Im $\alpha$, + est un SEV de K, W, +.

\subsubsection{Autre définition}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, alors on appelle \textit{rang de $\alpha$}, la dimension de Im $\alpha$.

\subsubsection{Théorème 6 (Très important)}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\alpha : V \rightarrow W$ est une AL, et si K, V, + est de dimension \textit{finie} $n$ alors :
\begin{center}
L'image d'une base par $\alpha$ est une partie génératrice de Im $\alpha$.
\end{center}

\subsection{Lien entre noyau et image d'une AL}
\subsubsection{Théorème 7 (Fondamental)}
Si K, V, + et K, W, + sont deux espaces vectoriels définis sur un corps K,  et si $\sigma : V \rightarrow W$ est une AL,et si V est de dimension \textit{finie} $n$, alors :
\begin{center}
dim(ker $\sigma$) + dim(Im $\sigma$) = dim V
\end{center}

\subsection{Formes linéaires - Espace dual}
\subsubsection{Définitions et propriétés}
Si K, V, + est un espace vectoriel défini sur un corps K commutatif, alors l'ensemble des formes linéaires de V dans K est appelé l'\textit{espace dual} de V et est noté $V^{*}$.\\
Autrement dit : $V^* = L(V, K)$.

\subsubsection{Exemples de formes linéaires}
\textit{Cf. cours}

\subsubsection{Forme linéaire et noyau}
\textbf{Théorème 1}\\
Si K, V, + est un espace vectoriel défini sur un corps K commutatif et si $f : V \rightarrow K$ est une forme linéaire sur V, alors :
\begin{center}
Soit ker($f$) est un hyperplan de V, soit ker($f$) = V
\end{center}
\ \\
\textbf{Défintion 3 :}\\
Si K, V, + est un espace vectoriel défini sur un corps K commutatif. Une forme linéaire $f : V \rightarrow K$ est \textit{non dégénérée} $\Leftrightarrow$
$$ker(f)\ est\ un\ hyperplan\ de\ V$$
La forme linéaire sera donc \textit{dégénérée} $\Leftrightarrow ker(f) = V$.
\subsubsection{Interprétation géométrique des forme linéaires}
Ne fait pas partie de l'examen.

\subsubsection{Expression d'une forme linéaire par rapport à une base de V}
Si K, V, + est un espace vectoriel de dimension finie défini sur un corps K commutatif, alors toute forme linéaire $f : V \rightarrow L$ est \textit{univoquement déterminée} par les valeurs qu'elle prend sur les éléments d'une base de V.

\subsubsection{Base de $K, V^*, +$ en dimension finie}
Si K, V, + est un espace vectoriel de dimension finie $n$ défini sur un corps K commutatif, et muni d'une base $e = (\vec{e_1}, \vec{e_2}, ..., \vec{e_n})$ alors les $n$ applications :
$$e_i^* : V \rightarrow K : \vec{x} = \sum_{j=1}^{n} x_j \vec{e_j} \rightarrow x_i = i^{ème}\ composante\ de\ \vec{x}\ dans\ la\ base\ e$$
forment une base de $K, V^*, +$, appelée \textit{base duale de la base e} qui sera notée $e^* = (\vec{e^*_1}, \vec{e^*_2}, ..., \vec{e^*_n})$\\
\ \\
\textbf{Propriété 1 (essentielle)}\\
Si K, V, + est un espace vectoriel de dimension finie $n$ défini sur un corps K commutatif, et muni d'une base $e = (\vec{e_1}, \vec{e_2}, ..., \vec{e_n})$ et si $e^* = (\vec{e^*_1}, \vec{e^*_2}, ..., \vec{e^*_n})$, alors :
$$\forall i, j = 1, ..., n = e_i^*(\vec{e_j}) = \delta_{ij}$$
(où $\delta_{ij}$ est le symbole de Kronecker)

\subsubsection{Composante d'une forme linéaire dans une base de $K, V^*, +$}
Partie essentiellement pratique (\textit{cf. TP}).

\newpage
\section{Rang de matrices - Systèmes d'équations linéaires}
\subsection{Rang d'une matrice}
Cette partie est fort \textit{recopiage} mais il n'y a pas vraiment le choix.
\subsubsection{Rang d'un système de vecteurs et rang d'une matrice}
\textbf{Définition 1} :\\
Le \textit{rang d'un système de vecteurs} d'un espace vectoriel K, V, + est la dimension du sous-espace vectoriel de $V$, engendré par cet ensemble de vecteurs.\\
\\
\textbf{Définition 2} : \\
Si $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif,alors le \textit{rang de A} est le nombre maximale de \textit{"colonnes de A linéairement indépendantes"}, c'est-à-dire de manière plus précise que le \textit{rang de A} est le rang du système de vecteurs-colonnes déterminé par les $n$ colonnes de $A$.\\
\\
\textit{NB :} Plus simplement (et moins mathématiquement du coup \ ;D), on peut considérer le le rang de A est le nombre de \textit{colonnes} linéairement indépendantes.

\subsubsection{Détermination du rang d'une matrice A d'ordre (p,n)}
C'est long et peu utile ici, regardez les slides ou elle donne un bon gros exemple bien détaillé !

\subsubsection{Propriété des rangs de matrices}
\textit{Si A est une matrice d'ordre $p \times n$} à coefficients dans un corps K commutatif, alors  :
$$rang(A) \leq n$$
\textit{Si A est une matrice d'ordre $p \times n$} à coefficients dans un corps K commutatif, alors :
$$rang(A) = rang( ^t A)$$
\textbf{Corollaire 1 :}\\
i $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif,alors :
$$rang(A) \leq min{p,n}$$
C'est 'logique' dans le sens ou une matrice de trois colonnes ne peut avoir quatre (et plus ) vecteurs LI.\\
\\
\textbf{Corrolaire 2 :}\\
Le rang d'une matrice $A$ ) coefficients dans un corps K commutatif peut se calculer aussi bien à partir d'un système de vecteurs-ligne associé à cette matrice qu'à partir du système de vecteurs-colonne associé.\\
\\
\textbf{Théorème :}\\
Si $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif,alors :
$$rang(A) = dim(Im \alpha)$$
ou $\alpha : V \rightarrow W$ est une application linéaire, K, V, + est une EV de dimension $n$ et de base $e$, K, W, + est une EV de dimension $p$ et de base $\epsilon$ et la matrice $\alpha$ dans les bases $e$ et $\epsilon$ est la matrice donnée $A$.\\
\\
\textbf{Lien avec les déterminants :}
Si $A$ est une matrice d'ordre $p \times n$  à coefficients dans un corps K commutatif. Considérons l'ensemble $\lbrace A_i\ \vert\ i=1,...,min\{p,n\}\rbrace$ de toutes les sous-matrices carrées extraites de $A$. On peut alors démontrer que :
$$rang(A) = max\ \lbrace\ ordre\ A_i\ \vert\ det(A_i) \neq 0 ; i = 1,...,min\{p,n\}\ \rbrace$$
\\
En français abusif, on peut dire que : '\textit{le rang de A est l'ordre du plus gros déterminant non nul que l'on peut extraire de la matrice A}'.\\\\
\textit{NB :} C'est généralement une mauvaise idée de faire ça, il faut plus l'utiliser quand on est presque à la fin si ça peut faire gagner du temps.

\newpage
\section{Formes bilinéaires et produits scalaires}
\subsection{Formes bilinéaires}
\subsubsection{Définitions}
Si V, + est un espace vectoriel réel de dimension finie, sur un corps K commutatif, on appelle \textbf{forme bilinéaire sur V} toute application 
$$f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$$
tel qu'elle est linéaire à gauche et à droite\\

On dira qu'une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{symétrique} $\Leftrightarrow \forall \vec{x}, \vec{y} \in V : f(\vec{x}, \vec{y}) = f(\vec{y}, \vec{x})$.

\subsubsection{Exemples de formes bilinéaires}
\textbf{Produit scalaire usuel}\\
Soit $V = \underline{\mathbb{R}^n}$, on appelle produit scalaire usuel l'application :
$$< , > : \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} = (\vec{x}, \vec{y}) \rightarrow <\vec{x}, \vec{y}> = \sum_{i=1}^n x_iy_i$$
avec $\vec{x} = (x_1, x_2, ..., x_n) \in \mathbb{R}^n$ et $\vec{y} = (y_1, y_2, ..., y_n) \in \mathbb{R}^n$.

\textbf{Attention :} il ne nécessite aucune base, les vecteurs sont des n-uples réels.\\

\emph{Théorème} : 
Le produit salaire usuel est une forme bilinéaire \textbf{symétrique} sur le corps des réels.

\subsection{Espace vectoriel des formes bilinéaires}
Soit \textit{Bil}(V) \textbf{l'ensemble des formes bilinéaires} sur un espace vectoriel K,V,+. \\
Par conséquent :
$f \in \mathcal{B}il(V)$ signifie que $f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$\\
Si on munit $\mathcal{B}il(V)$ d'une addition vectorielle et d'une multiplication par un scalaire (\textit{def p. 161}) alors $K,\mathcal{B}il(V),+$ est un espace vectoriel.

\subsubsection*{Théorème}
Si V, + est un espace vectoriel de dimension finie n sur un corps K commutatif et si $e = (\vec{e_1}, ..., \vec{e_n})$ est une base de V, alors les $n^2$ formes bilinéaires 
$$g_{ij} : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow g_{ij}(\vec{x}, \vec{y}) = e_i^*(\vec{x})e_j^*(\vec{y})\ (\forall i, j : 1,2,...n)$$
forment une base de $K,\mathcal{B}il(V),+$.\\

\emph{Corolaire}\\
Soient V, + est un espace vectoriel de dimension finie n sur un corps K commutatif muni d'une base $e = (\vec{e_1}, ..., \vec{e_n})$ et $f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$ une forme bilinéaire définie sur V, alors
$$f = \sum_{i=1}^n\sum_{j=1}^n f(\vec{e_i}\vec{e_j})g_{ij}$$
et les composantes de $f$ dans la base $(j_{ij} | i,j= 1,2,...n)$ de \textit{Bil}(V) sont \textbf{les images par f des couples de vecteurs de la base e}.(\textit{Bon exemple page 165}).

\subsection{Matrice d'une forme bilinéaire dans une base donnée}
Pour aller droit au but, en considérant la base canonique $e = (\vec{e_1}, \vec{e_2}, \vec{e_3})$, \textbf{la matrice $F^e$ de la forme bilinéaire $f$ dans la base $e$} :
$$\begin{pmatrix}
f(\vec{e_1}, \vec{e_1}) & f(\vec{e_1}, \vec{e_2}) & f(\vec{e_1}, \vec{e_3})\\
f(\vec{e_2}, \vec{e_1}) & f(\vec{e_2}, \vec{e_2}) & f(\vec{e_2}, \vec{e_3})\\
f(\vec{e_3}, \vec{e_1}) & f(\vec{e_3}, \vec{e_2}) & f(\vec{e_3}, \vec{e_3})\\
\end{pmatrix}$$
Connaissant deux vecteurs $\vec{x}$ et $\vec{y}$, on peut calculer $f(\vec{x}, \vec{y})$ de deux façons différentes : en remplaçant directement dans l'expression ou de façon matricielle :$f(\vec{x}, \vec{y}) =\ ^tX_e F^e Y_e$.
$$f(\vec{x}, \vec{y})\ =\ \left(x_1, x_2, x_3\right)\begin{pmatrix}
f(\vec{e_1}, \vec{e_1}) & f(\vec{e_1}, \vec{e_2}) & f(\vec{e_1}, \vec{e_3})\\
f(\vec{e_2}, \vec{e_1}) & f(\vec{e_2}, \vec{e_2}) & f(\vec{e_2}, \vec{e_3})\\
f(\vec{e_3}, \vec{e_1}) & f(\vec{e_3}, \vec{e_2}) & f(\vec{e_3}, \vec{e_3})\\
\end{pmatrix}\begin{pmatrix}
y_1\\
y_2\\
y_3\\
\end{pmatrix}$$

\subsection{Formes bilinéaires et changements de bases}
Le lien entre la matrice $F^e$ de $f$ dans la base $e$ et la matrice $F^u$ de $f$ dans la base $uu$ est :
$$F^u =\ ^t(P_e^u)F^eP^u_e$$
\emph{Définition}\\
Le \textbf{rang d'une forme bilinéaire} $f : V \times V \rightarrow K$ est le rang de la matrice de $f$ dans une base quelconque de V.

\subsection{Produits scalaires}
\subsubsection{Définitions}
\emph{Définition 1}\\
Si V, + est un espace vectoriel \textbf{réel} on appelle \textbf{produit scalaire sur V} tout \textit{forme bilinéaire symétrique} sur V. On utilisera pour les produits scalaire la notation qui suit :
$$< , > : V \times V \rightarrow \mathbb{R} = (\vec{x}, \vec{y}) \rightarrow <\vec{x}, \vec{y}>$$\\

\emph{Définition 2}\\
Un produit scalaire $< , >$ défini sur un espace vectoriel $\mathbb{R}, V, +$ est \textbf{défini positif} $\Leftrightarrow\  < , >$ est un produit scalaire tel que :
\begin{enumerate}
\item $\forall \vec{x} \in V : <\vec{x}, \vec{x}> \geq 0\ \ (positif)$
\item $< \vec{x}, \vec{x}> = 0 \Leftrightarrow \vec{x} = \vec{0}$
\end{enumerate}
\subsubsection{Exemples de P.Scal (à connaître)}
\begin{enumerate}
\item Le produit scalaire \textbf{usuel} (voir plus haut)
\item $< , > : \mathcal{P}_3 \times \mathcal{P}_3 \rightarrow \mathbb{R} : (\vec{p}, \vec{q}) \rightarrow <\vec{p}, \vec{q}> = \int_0^1 p(t)q(t) dt$
\item $< , > : \mathbb{R}^{p \times n} \times \mathbb{R}^{p \times n} \rightarrow \mathbb{R} : (A, B) \rightarrow <A, B> = tr(A\ \ ^tB)$
\end{enumerate}

\subsection{Espaces euclidiens centrés}
Un espace \textbf{euclidien centré} est un espace vectoriel \textbf{réel} muni d'un produit scalaire \textbf{défini positif} noté $\mathbb{R},V,+, < , >$.

\subsection{Propriété de formes bilinéaires particulières}
\textbf{Définition 1} : Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{symétrique} ssi $\forall \vec{x}, \vec{y} \in V = f(\vec{x}, \vec{y}) = f(\vec{y}, \vec{x})$.\\

\textbf{Théorème 1} :
Une forme bilinéaire $f : V \times V \rightarrow K$ est symétrique ssi il existe une base $e$ de $V$ dans laquelle la matrice de $f$ est symétrique.\\

\textbf{Définition 2} : Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{antisymétrique} ssi $\forall \vec{x}, \vec{y} \in V = f(\vec{x}, \vec{y}) = -f(\vec{y}, \vec{x})$.\\

\textbf{Définition 3} : Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{alternée} ssi $\forall \vec{x} \in V : f(\vec{x}, \vec{x}) = 0$.\\

\textbf{Théorème 2} : Si $2 \neq 0$ dans K, alors tout forme bilinéaire antisymétrique est alternée.\\

\textbf{Théorème 3} : Toute forme bilinéaire alternée est antisymétrique.\\

\textbf{Définition 1} : Si $2 \neq 0$ dans K, alors toute forme bilinéaire $f : V \times V \rightarrow K$ est la somme d'une forme bilinéaire symétrique et d'une forme bilinéaire alternée.\\

\subsection{Orthogonalité}
\subsubsection*{Définition}
Soit K, V, + un EV de dimension finie, défini sur un corps K commutatif et $f : V \times V \rightarrow K$ une forme bilinéaire sur $V$. On dira qu'un \textbf{vecteur} $\vec{x} \in V$ est \textbf{orthogonal} à $\vec{y} \in V\ \ (\vec{x} \perp \vec{y}) \Leftrightarrow$
$$f(\vec{x}, \vec{y}) = 0$$
Hélas, $\perp$ n'est pas toujours symétrique : si $f(\vec{x}, \vec{y}) 0 = 0 \neq f(\vec{y},\vec{x})$ on risque d'avoir un souci ! Sont-ils perpendiculaires ou non ? Pour régler le souci, il y a la \textit{réflexivité}.

\subsubsection*{Définition}
Une forme bilinéaire $f : V \times V \rightarrow K$ est \textbf{réflexive} 
$$\Leftrightarrow \forall \vec{x}, \vec{y} \in V \left[ \vec{x} \perp \vec{y} \Rightarrow \vec{y} \perp \vec{x}\right]$$
Ainsi, une forme bilinéaire est réflexive $\Leftrightarrow$ \textbf{f est symétrique ou alternée}.\\

Hélas, il peut exister des vecteurs non nuls de $V$ qui sont orthogonaux à eux mêmes.
\subsubsection*{Définition}
Soit K, V, + un EV de dimension finie, défini sur un corps K commutatif et $f : V \times V \rightarrow K$ une forme bilinéaire sur $V$. \\
On appelle \textbf{vecteur isotrope de V}, tout vecteur $\vec{x} \in V$ tel que $f(\vec{x}, \vec{y}) = 0$.\\

Pire, s'il existe au moins un vecteur non nul de $V$ qui est orthogonal à tous les vecteurs de $V$ on dira que $f$ est \textbf{dégénérée}. Dans le cas contraire (ou il n'y en a aucun) f est dite \textbf{non dégénérée}.

\subsubsection*{Théorème}
En passant le blabla de la base et de l'EV, si $F^e$ est la matrice d'une forme bilinéaire dans la base $e$ alors f est \textbf{dégénérée} 
$$\Leftrightarrow det(F^e) = 0\ \ \Leftrightarrow rang\ de\ f\ < n$$

\newpage
\section{Produits hermitiens}
\subsection{Produits hermitiens ou forme hermitiennes}
Si $V, +$ est un espace vectoriel \underline{complexe} ($K = \mathbb{C}$) de dimension finie, on appelle \textbf{produit hermitien} ou \textbf{forme hermitienne sur V} tout application $h : V \times V \rightarrow \mathbb{C} : (\vec{x}, \vec{y}) \rightarrow h(\vec{x}, \vec{y})$ telle que $\forall \vec{x}, \vec{y}, \vec{z} \in V\ et\ \forall \lambda \in \mathbb{C} :$
\begin{enumerate}
\item $h(\vec{x} + \vec{y}, \vec{z}) = h(\vec{x}, \vec{z}) + h(\vec{y}, \vec{z})$
\item $h(\lambda\vec{x}, \vec{z}) = \lambda h(\vec{x}, \vec{z})$
\item $h(\vec{x}, \vec{z}) = \overline{h(\vec{z}, \vec{x})}$
\end{enumerate}
où $\overline{h(\vec{z}, \vec{x})}$ désigne le conjugué de $h(\vec{z}, \vec{x})$ dans $\mathbb{C}$.\\

La condition \textit{3.} implique la \textbf{semi-linéarité à droite} de $h$:\\
$\forall \vec{x}, \vec{y}, \vec{z} \in V\ et\ \forall \lambda \in \mathbb{C} :$
\begin{enumerate}
\item $h(\vec{x}, \vec{y} + \vec{z}) = h(\vec{x}, \vec{y}) + h(\vec{x}, \vec{z})$
\item $h(\vec{x}, \lambda\vec{z}) = \overline{\lambda}h(\vec{x}, \vec{z})$
\end{enumerate}
Cette deuxième implication expliquera la présence du conjugué à droite dans l'écriture matricielle du produit hermitien. \\
\textit{NB :} On général, on désigne les produits hermitiens par $< , >$ et non par $h$.

\subsubsection*{Définition 2 (A connaître par keur-keur)}
Soit $V = \mathbb{C}^n$, on appelle \textbf{produit hermitien usuel} l'application :
$$< , > : \mathbb{C}^n  \times \mathbb{C}^n \rightarrow \mathbb{C} : (\vec{x}, \vec{y}) \rightarrow <\vec{x}, \vec{y}> = \sum_{i=1}^n x_i\overline{y_i}$$
où $\vec{x}$ et $\vec{y}$ appartiennent à $\mathbb{C}$.

\subsection{Matrice d'un produit hermitien dans une base donnée}
C'est exactement la même chose que pour le produit scalaire si ce n'est qu'on utilise le produit hermitien.
$$G^e = \left( <\vec{e_i}, \vec{e_j}>\right)$$

On calcule ainsi le produit hermitien de la façon suivante : 
$$<\vec{x}, \vec{y}> = \left(^t X_e\right)G^e\left(\overline{Y_e}\right)$$
\textbf{Attention :} on conjugue les composantes de $\vec{y}$ !

\subsubsection*{Théorème}
On dira qu'une application dans une base $e$ est un produit hermitien sur $V$ ssi $H^e$ est un matrice hermétique ($H = ^t(\overline{A})$).

\subsection{Matrice d'un produit hermitien et changement de base}
Semblable aux produits scalaires, il ne faut juste pas oublier la conjuguée
$$G^u =\ ^t(P^u_e) G^e (\overline{P^u_e})$$

\subsection{Produits hermitiens définis positifs}
Tatatitatata sera défini positif ssi :
\begin{enumerate}
\item $\forall \vec{x} \in V : <\vec{x}, \vec{x}> \in \mathbb{R}$
\item $\forall \vec{x} \in V : \vec{x}, \vec{x}> \geq 0$
\item $\forall \vec{x} \in V : \vec{x}, \vec{x}> = 0 \Leftrightarrow \vec{x} = \vec{0}$
\end{enumerate}
Par propriété, le produit hermitien usuel est défini positif.

\subsection{Espaces hermitiens centrés}
\textbf{Un espace hermitien centré} (ou \textbf{espace préhilbertien complexe}) est un espace vectoriel complexe muni d'un produit hermitien défini positif.

\newpage
\section{Orthogonalité et normes}
\subsection{Espaces euclidiens et hermitiens centrés}
Un espace \underline{euclidien centré} est un espace vectoriel réel muni d'un produit scalaire défini positif.\\
Un espace \underline{hermitien centré} est un espace vectoriel complexe muni d'un produit hermitien défini positif.

\subsection{Sous-espace orthogonaux}
\subsubsection*{Définition 1}
Si $V, < , >$ est un espace euclidien ou hermitien centré, alors deux vecteur $\vec{x}$ et $\vec{y}$ de $V$ sont \textbf{orthogonaux} \textit{pour} $< , >$ ssi $<\vec{x}, \vec{y}> = 0$.

\subsubsection*{Définition 2}
Si $S$ est un sous-ensemble non vide d'un espace euclidien ou hermitien centré $V, < , >$, alors 
$$S^{\perp} =  \left\{\vec{y} \in V | <\vec{x}, \vec{y}> = 0\ \forall \vec{x} \in S \right\} = l'orthogonal\ de\ S\ pour\ <\ ,\ >$$

\subsubsection*{Propriétés}
Si $S$ est un sous-espace vectoriel d'un espace euclidien ou hermitien centré V, < , >, alors $S^{\perp}$ est aussi un sous-vectoriel de V, < , >. On appelle alors $S^{\perp}$ le \textbf{sous-espace orthogonal de} $S$.\\

Si $S$ est un sous-espace vectoriel d'un espace euclidien ou hermitien centré V, <, >, alors 
$$dim(S) + dim(S^\perp) = dim(V)$$.\\

Si $S$ est un sous-ensemble non vide d'un espace euclidien ou hermitien centré V, <, > alors $S \cap S^\perp = \{\vec{0}\}\ si\ \vec{0} \in S\ ou\ \phi\ si\ \vec{0} \notin S$.\\

Si $S = V$ alors $S^\perp = \{\vec{0}\}$. ; le vecteur nul est le seul vecteur d'un espace euclidien (ou hermitien) centré orthogonal à tous les vecteurs de l'espace.\\

Si $V, <, >$ est un espace euclidien ou hermitien centré, alors tout sous-ensemble de vecteur \textbf{non nuls, orthogonaux deux à deux} forme une partie libre de $V$.

\subsection{Projection orthogonale - Coefficient de Fourier}
\subsubsection*{Définition 3}
Si $V, < , >$ est un espace euclidien ou hermitien centré, et si $\vec{x}$ et $\vec{y}$ sont deux vecteurs \textit{non nuls} de V, le \textbf{coefficient de Fourrier de $\vec{x}$ par rapport à $\vec{y}$} vaut : 
$$\lambda = \frac{\langle\vec{x}, \vec{y}\rangle}{\langle\vec{y}, \vec{y}\rangle}$$

Si $V, \langle , \rangle$ est un espace euclidien ou hermitien centré, et si $\vec{x}$ et $\vec{y}$ sont deux vecteurs \textit{non nuls} de V,la \textbf{projection orthogonale de $\vec{x}$ su $\vec{y}$} est le \textsc{vecteur} :
$$\vec{p} = \frac{\langle\vec{x}, \vec{y}\rangle}{\langle\vec{y}, \vec{y}\rangle}\vec{y}$$

\subsubsection{Théorème important :}
Si V, <, > est un espace euclidien ou hermitien centré, et si $W$ est un sous-espace de dimension $k$ de $V$, $W$ étant muni d'une base $(\vec{e_1'}, ..., \vec{e_k'})$ orthogonale pour $\langle , \rangle$ alors \textbf{la projection orthogonale d'un vecteur $\vec{x} \in V - W$ sur $W$} est égale au vecteur 
$$\vec{p} = \sum_{i=1}^k \frac{\langle\vec{x}, \vec{e_i'}\rangle}{\langle \vec{e_i'}, \vec{e_i'}\rangle}\vec{e_i'}$$
Ce vecteur est la somme des projections orthogonales du vecteur $\vec{x}$ sur chacun des vecteurs de la base $e'$.\\

De façon plus générale, si $\vec{x} = \sum_{i=1}^n x_i\vec{e_i}$, alors : ( où $x_j = \frac{\langle\vec{x}, \vec{e_j} \rangle}{\langle\vec{e_j}, \vec{e_j}\rangle}$) :
$$X_e = \begin{pmatrix} 
x_1\\
\vdots\\ 
x_j\\
\vdots\\
x_n \end{pmatrix}$$
\textbf{Voir conclusion page 211}.

\subsubsection*{Définition 5}
Soit $\mathbb{R}, V, +, \langle , \rangle$ un espace euclidien ou hermitien centré de dimension finie $n$. Une \textbf{base} $e = (\vec{e_1}, ... \vec{e_n})$ est \textbf{orthonormée pour le produit $\langle , \rangle$}
$$\Leftrightarrow \langle\vec{e_i}, \vec{e_j}\rangle = \delta_{ij}$$

\subsection{Expression d'un produit scalaire dans une base orthonormée}
\subsubsection{Théorème (très très important)}
Si $K , V, +, \langle , \rangle$ est un espace euclidien ou hermitien centré de dimension finie $n$, muni d'une base $e$ \textbf{orthonormée pour le produit $\langle , \rangle$} alors 
$$\forall \vec{x}, \vec{y} \in V : \langle \vec{x}, \vec{y}\rangle = \sum_{i=1}^n x_i\overline{y_i}$$
où $X_e$ est la matrice des composantes de $\vec{x}$ dans la base orthonormée $e$ et $Y_e$ est la matrice des composantes de $\vec{y}$ dans la base orthonormée $e$.\\

En français : Certains produits scalaires peuvent être assez contraignants à calculer. Mais, si l'on construit une base orthonormée pour ce \textit{moche} produit, en travaillant dans cette nouvelle base le \textit{moche} produit \textbf{se ramène au produit scalaire ou hermitien usuel à partir des composantes de ces vecteurs dans la base orthonormée trouvée} ce qui simplifie les calculs ! Houra, me direz-vous.


\subsubsection*{Application importante : Théorème}
$$\forall \vec{x}, \vec{y} \in V : \pscal{\alpha(\vec{x})}{\vec{y}} = \pscal{\vec{x}}{\alpha(\vec{x})}$$
Si ce théorème est vérifié, on dira que $\alpha$ est \textbf{auto-adjoint} (ou hermitien).

\subsection{Algorithme d'orthogonalisation de Gram - Schmidt}
Partie essentiellement pratique, \textit{cf. TP}.

\subsection{Norme euclidienne ou hermitienne}
Si $V, \langle , \rangle$ est un espace euclidien (ou hermitien) centré, on appelle \textbf{norme euclidienne} (... hermitienne) \textbf{associée à} $\langle , \rangle$ toute application : 
$$||\ ||  : V \rightarrow \mathbb{R}^+ : \vec{x} \rightarrow ||\vec{x}|| = \sqrt{\pscal{\vec{x}}{\vec{x}}}$$

\subsubsection{Propriété de la norme euclidienne (ou hermitienne)}
C'est relativement simple, \textit{cf. page 221 - 227}.

\subsection{Norme généralisée}
Si $K, V, +$ est un espace vectoriel réel ou complexe, on appelle \textbf{borme de $V$} toute application $||\ || : V \rightarrow \mathbb{R}^+ : \vec{x} \rightarrow ||\vec{x}||$ telle que : 
\begin{enumerate}
\item $\forall \vec{x} \in V : ||\vec{x}|| \geq 0$ et $\left[||\vec{x}|| = 0 \Leftrightarrow \vec{x} = \vec{0}\right]$
\item $\forall \vec{x} \in V$ et $\forall \lambda \in K : ||\lambda\vec{x}|| = |\lambda |\ ||\vec{x}||$
\item $\forall \vec{x}, \vec{y} \in V : ||\vec{x} + \vec{y}|| \leq ||\vec{x}|| + ||\vec{y}||$\ \ (inégalité triangulaire)
\end{enumerate}

\section{Formes quadratiques}
A partir d'ici, cette synthèse comprend seulement ce qui à une application aux TP, cela ne sert à rien de couvrir plus (Cf. la prof). En gros la matière théorique de l'examen de juin porte sur la matière des TP.

\subsection{Définition - forme polaire}
Si V, + est un espace vectoriel de dimension finie sur un corps K commutatif, tel que $0 \neq 2$, et si $f : V \times V \rightarrow K : (\vec{x}, \vec{y}) \rightarrow f(\vec{x}, \vec{y})$ est une forme bilinéaire \textbf{symétrique}, alors on appelle \textbf{forme quadratique sur V associée à} $f$ l'application $q : V \rightarrow K : \vec{x} \rightarrow q(\vec{x}) = f(\vec{x}, \vec{x})$.\\

Ainsi, toute forme bilinéaire symétrique définit une forme quadratique et réciproquement, la donnée d'une forme quadratique permet de reconstituer univoquement la forme bilinéaire associée.

\subsection{Formes quadratiques réelles}
Il existe toujours une base $u$ de V telle que $Q^u = F^u$ est une matrice \textbf{diagonale}.

\subsection{Réduction de Gauss}
Voir séance de TP, mais il est très important d'appliquer l'algorithme \textsc{à la lettre} !\\
Celle-ci permet de trouver une base dans laquelle la forme quadratique est diagonale.

\subsection{Loi d'inertie - Signature d'une forme quadratique}
Toute représentation en somme de carrés d'une forme quadratique réelle aura 
\begin{enumerate}
\item Un nombre constant $P_q$ de termes à coefficients strictement positifs
\item Un nombre constant $N_q$ de termes à coefficients strictement négatif
\end{enumerate}
La \textbf{signature de la forme quadratique q} est le couple $(P_q, N_q)$.\\

Le nombre de termes non nuls d'une représentation diagonale d'une forme quadratique réelle $q$ s'appelle le \textbf{rang de q}, qui est égal au rang de la matrice de $q$ dans n'importe quelle base de l'espace.

\newpage
\section{Valeurs propres et sous-espaces propre d'un opérateur linéaire}
\subsection{Définitions}
Si V, + est un espace vectoriel sur un corps K commutatif, et si $\alpha : V \rightarrow V$ est un opérateur linéaire de V, alors on appelle \textbf{vecteur propre de $\alpha$} tout vecteur $\vec{x}$ de V tel que $\exists \lambda \in K\ |\ \alpha(\vec{x}) = \lambda\vec{x}$.\\

\textbf{Définition 2}\\
Si V, + est un espace vectoriel sur un corps K commutatif, et si $\alpha : V \rightarrow V$ est un opérateur linéaire de V, alors un \textbf{scalaire} $\lambda \in K$ est une \textbf{valeur propre de $\alpha$} ssi il \textbf{existe} un vecteur $\vec{x} \in V$ \textbf{non-nul} tel que $\alpha(\vec{x}) = \lambda \vec{x}$.\\

\textbf{Définition 3}\\
Le \textbf{sous espace propre de $\alpha$ associé à la valeur propre $\lambda$} est l'ensemble de tous les vecteurs propres de $\alpha$ associé à la valeur propre $\lambda$
$$W_\lambda = \left\lbrace \vec{x} \in V\ |\ \alpha(\vec{x}) = \lambda\vec{x}\right\rbrace$$\\

\textbf{Définition 4}\\
L'ensemble des valeurs propres de $\alpha$ s'appelle le \textbf{spectre de l'opérateur $\alpha$}.

\subsection{Valeurs propres particulières}
\begin{itemize}
\item[$\lambda = 1$] L'ensemble des valeurs propre de $\alpha$ associée à $\lambda = 1$ est l'ensemble des \textit{points fixes de $\alpha$}.
\item[$\lambda = 0$] L'ensemble des valeurs propre de $\alpha$ associée à $\lambda = 0$ est \textit{le noyau $ker(\alpha)$ de $\alpha$}.
\item[$\lambda = 0$] L'application $\alpha$ n'est pas injective (il faut en effet qu'il existe un vecteur non nul, ce qui n'est pas le cas ici)
\end{itemize}

\end{document}